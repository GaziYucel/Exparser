<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta />
    <article-meta>
      <title-group>
        <article-title>Improving Retrieval Results with Discipline-Specific Query Expansion</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>Thomas Lüke</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Philipp Schaer</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Philipp Mayr</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>GESIS - Leibniz Institute for the Social Sciences</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Unter Sachsenhausen</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Cologne</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Germany</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>thomas.lueke</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>philipp.schaer</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>philipp.mayr}@gesis.org</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Keywords: Term Suggestion</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Information Retrieval</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Thesaurus</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Query Expan- sion</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Digital Libraries</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Search Term Recommendation.</string-name>
        </contrib>
      </contrib-group>
      <fpage>408</fpage>
      <lpage>413</lpage>
      <abstract>
        <p>Choosing the right terms to describe an information need is becoming more difficult as the amount of available information increases. SearchTerm-Recommendation (STR) systems can help to overcome these problems. This paper evaluates the benefits that may be gained from the use of STRs in Query Expansion (QE). We create 17 STRs, 16 based on specific disciplines and one giving general recommendations, and compare the retrieval performance of these STRs. The main findings are: (1) QE with specific STRs leads to significantly better results than QE with a general STR, (2) QE with specific STRs selected by a heuristic mechanism of topic classification leads to better results than the general STR, however (3) selecting the best matching specific STR in an automatic way is a major challenge of this process.</p>
      </abstract>
    </article-meta>
  </front>
  <body>
    <sec id="sec-1">
      <title>Introduction</title>
      <p>language specialization to recommend the most specific concepts from a controlled
vocabulary through discipline-specific STRs. We conduct an empirical analysis in the
domain of social sciences to evaluate retrieval performance in a standard IR
evaluation environment using QE.</p>
      <p>The paper is structured as follows: Section 2 gives a brief overview of previous
findings in the area of search term recommendation, query expansion and IR
evaluation. Section 3 describes the methods used in this paper to evaluate our setup. In
Section 4 we present and discuss our evaluation. Section 5 summarizes the findings of
this paper and presents ideas for future work.
2</p>
    </sec>
    <sec id="sec-2">
      <title>Related Work</title>
      <p>
        Hargittai [
        <xref ref-type="bibr" rid="ref2">2</xref>
        ] has shown that users need supporting mechanisms while expressing their
information need through search queries. Such support may be provided by query
recommendation mechanisms, which try to enrich the existing query with additional
terms. This leads to better retrieval results or provides the searcher with a new
viewpoint on the search as shown by Mutschke et al. [
        <xref ref-type="bibr" rid="ref5">5</xref>
        ].
      </p>
      <p>
        Automatic Query Expansion mechanisms have been divided into two classes by
Xu and Croft [
        <xref ref-type="bibr" rid="ref8">8</xref>
        ]: Expansion recommendations based on a global analysis of the entire
document collection and recommendations based on the local subset of documents
that were retrieved by using the unexpanded query (called pseudo-relevance feedback
[
        <xref ref-type="bibr" rid="ref4">4</xref>
        ]). In their research the local approach outperformed the global one clearly.
However, when the amount of non-relevant documents in the results to the original query
increases, a so-called query drift may occur. Documents not relevant to the users’
information need lead to mostly irrelevant expansion terms. If the query gets
expanded with such terms, it drifts away from the original meaning and results in even
less relevant documents. Mitra et. al [
        <xref ref-type="bibr" rid="ref4">4</xref>
        ] have proposed techniques to overcome query
drift.
      </p>
      <p>
        Additionally Petras [
        <xref ref-type="bibr" rid="ref7">7</xref>
        ] found that discipline-specific search term recommenders
which are trained on sub disciplinary document corpora deliver more specific search
term suggestions than general recommenders which are trained on an entire database.
3
      </p>
    </sec>
    <sec id="sec-3">
      <title>Discipline-Specific Term Recommendation</title>
      <p>
        In our approach we apply Petras’ [
        <xref ref-type="bibr" rid="ref7">7</xref>
        ] idea of more specific STRs onto an an automatic
query expansion setup. According to the findings in [
        <xref ref-type="bibr" rid="ref4 ref8">4, 8</xref>
        ] we expect retrieval results
to improve. With real-world applications in mind and the intention to reduce the
chance of query drift through non-relevant data sets we demand STRs to be created
apriori rather than on-demand as with pseudo-relevance feedback. To create document
sets belonging to specific disciplines we use a hierarchically structured classification
system. It is called “classification of social sciences” and part of the SOLIS1 data set.
1
http://www.gesis.org/en/services/research/solis-social-scienceliterature-information-system/
For example any class starting with 1 is connected to the entire field of social
sciences, any class starting with 102 is connected to the sub-discipline of sociology
and class 10209 is assigned to documents from the special field of family sociology.
This structure allowed us to create STRs at the top level that cover every area of the
classification system thereby allowing us to choose matching discipline-specific STRs
for queries from various disciplines.
      </p>
      <p>Table 1. Overview of the created discipline-specific (DS) STRs. Class describes the specific
sub-discipline of the social sciences the STR was based on. #Docs and #CT shows the number
of documents or controlled vocabulary terms in that given collection.</p>
      <p>STR
DS-1
DS-2
DS-3
DS-4
DS-5
DS-6
DS-7
DS-8
Global</p>
      <p>Class
Basic Research
Sociology
Demography
Ethnology
Political
Science
Education
Psychology
Communications
Social sciences</p>
      <p>To test the effects of recommendation terms from different disciplines we use a
standard IR evaluation environment. A set of pre-defined topics, each consisting of a
title and a set of documents relevant to that query, is processed on the SOLR2 search
platform which uses tf-idf to rank results. The data sets that represent the basis for the
different STRs are all created from the SOLIS dataset, a collection of more than
400,000 documents from various disciplines of the social sciences.</p>
      <p>Based on the assumption of a specialized vocabulary in different scientific
disciplines, 16 custom data sets (see DS-1 to DS-16 in Table 1) for different sub-disciplines
of SOLIS are created. These 16 sets as well as the entire SOLIS data set (see Global
in Table 1) are the basis for 17 STRs.</p>
      <p>In addition SOLIS is indexed with the thesaurus TheSoz3 which consists of almost
7800 descriptor terms. Our discipline-specific STRs reduce the vocabulary of TheSoz
to about 5700 terms per data set (on average) with a trend of even smaller (and
presumably more specific) vocabularies. Exact numbers can be found in Table 1.</p>
      <p>Each STR is created to match arbitrary input terms to terms of the controlled
vocabulary TheSoz. All documents of a data set are processed using a co-occurrence
analysis of input terms (found in title and abstract of each document) and the
subjectspecific descriptor terms assigned to a document. In order to rank the suggested terms
from the controlled vocabulary the logarithmically modified Jaccard similarity
measure is used.
2 http://lucene.apache.org/solr/
3 http://thedatahub.org/dataset/gesis-thesoz</p>
    </sec>
    <sec id="sec-4">
      <title>Evaluation</title>
      <p>This section contains a description of our evaluation. In the first paragraph the setup is
described. The second paragraph presents the main results and effects of
disciplinespecific expansion on retrieval performance measured through average precision
values. In addition we give a single example of a query, going from the broader level to a
detailed in-depth inspection of discipline-specific QE.
4.1</p>
      <sec id="sec-4-1">
        <title>Evaluation Setup</title>
        <p>
          In order to test the effects of discipline-specific STRs and a general STR within a
query expansion scenario we choose the GIRT4 corpus [
          <xref ref-type="bibr" rid="ref3">3</xref>
          ], which is a subset of
SOLIS. It is used in evaluation campaigns like CLEF or TREC. We use 100 of the
CLEF topics ranging from years 2003 to 2006 (topic numbers were 76 to 175) and
classify them through a heuristic approach based on the classification system
mentioned above: All relevant documents for a given topic are put into groups based on
their classification ID. The classification ID of the group that holds the most
documents is assumed to be the topic’s classification. Queries from these topics are created
by removing stop words from the title of each topic. To test the performance of QE
we expand the query with three different STRs:
•
•
•
the general STR, based on the entire SOLIS data set (our baseline)
the STR of the topic’s class
the STR that performs best for a given topic (out of the 16 discipline-specific
STRs)
Every QE is performed automatically with the top 4 recommendations of a STR. We
report mean average precision (MAP), rPrecision as well as p@5, p@10, p@20 and
p@30. As additional comparison we include the results of a standard installation of
SOLR without QE. Every query, whether expanded or not, is processed by this
platform. We use Student’s t-test to verify significance of the improvements.
4.2
        </p>
      </sec>
      <sec id="sec-4-2">
        <title>Results</title>
        <p>
          Table 2 shows the results of the QE with different STRs. The first observation which
can be made is that the discipline-specific STRs always perform better than a general
STR (and thereby also improve retrieval performance compared to an unexpanded
query). The last line of Table 2 shows the maximum performance possible through
the use of our 16 discipline-specific STRs. It is significantly better than a general STR
in every case. However, the improvements for those STRs that are chosen based on
the classification of topics did not always reach significance. Only the precision
within the first 5 and 10 top ranked documents is significantly higher. Still a more precise
classification of the original query is necessary to gain maximum benefit from our
discipline-specific STRs. In addition to measuring the impact on average precision we
further analyze the use of discipline-specific STRs by examining an individual query.
According to Petras [
          <xref ref-type="bibr" rid="ref6">6</xref>
          ] the results of a QE could significantly improve if the “right”
Table 2. Evaluation results averaged over 100 topics for three different types of QE.
Recommendations are based on a general STR (gSTR) which served as a baseline, a
disciplinespecific STR fitting the class of the topic (tSTR) and the discipline-specific STR performing
best on each topic (bSTR). Confidence levels of significance are: * α = .05, ** α = .01.
Exp. Type MAP
gSTR (Base) 0.155
tSTR 0.159
bSTR 0.179**
rPrecison
0.221
0.224
0.233**
        </p>
        <p>Table 3. Top 4 recommendations for the input terms “bilingual education” from three STRs</p>
        <sec id="sec-4-2-1">
          <title>Recommendation</title>
          <p>1
2
3
4</p>
        </sec>
        <sec id="sec-4-2-2">
          <title>General</title>
          <p>Multilingualism
Child
Speech
Intercultural Education</p>
        </sec>
        <sec id="sec-4-2-3">
          <title>Topic-fitting</title>
          <p>Child
School
Multilingualism
Germany</p>
        </sec>
        <sec id="sec-4-2-4">
          <title>Best-performing</title>
          <p>Multilingualism
Speech
Ethnic Group
Minority
terms are added to the query. We will see how different recommendations influence
the results. Topic no. 131 has the title (and thus the query) “bilingual education”.
Table 3 shows the top 4 recommendations of each STR for this topic. While
“multilingualism” is always a recommendation and “child” and “speech” appear twice, the
rest of the terms appear only in one recommender. The general recommender
proposes the most common terms while the two discipline-specific STRs propose more
specific terms. In Table 4 we can see the effects of these different recommendations on
retrieval precision. The unexpanded query performs satisfying but leaves room for
improvement as it presents only 2 relevant documents within the top 5 and 3 within
the top 10 documents (see p@5 and p@10). Expanding the query with terms from the
general STR improves these results to 3 and 6 relevant documents in the top 5 or top
10 respectively. Using terms from the pre-defined, topic-fitting, discipline-specific
STR 4 out of 5 documents within the top 5 are relevant. Finally, the best performing
discipline-specific STR manages to expand the query in a way that all top 10
documents are relevant and even within the top 20 documents only 3 are not relevant.
Table 4. Evaluation results for topic 131 with three different types of QE Recommendations.
Bold font indicates improvement</p>
        </sec>
        <sec id="sec-4-2-5">
          <title>Exp. Type</title>
          <p>Solr
gSTR
tSTR
bSTR
5</p>
          <p>Conclusion and Future Work
p@10
0.3
0.6
0.6
1
p@20
0.2
0.4
0.45
0.85
p@30
0.133
0.333
0.333
0.567
Our research shows that the use of discipline-specific Search-Term-Recommenders
can improve the retrieval performance significantly if used as basis for an automated
query expansion. However, it also becomes clear that choosing the best STR in an
automated setting of query expansion is far from trivial. By doing an in-depth analysis
of a single query we additionally demonstrate how discipline-specific term
recommendations can improve the quality of search results for a user. This leads us to the
conclusion that discipline-specific STRs can be a valuable addition to expert search
platforms where users might not know how to optimally express their search.</p>
          <p>In conclusion, STRs that are meant to assist users should be discipline-specific in
order to recommend more specific terms. Still, it has to be determined how specific
(or small) a data set may be while still producing reasonable results. To improve
quality of QE it is essential to have a good algorithm for determining the specific
discipline of the query. Besides having more specific recommendation another aspect of
further research could be the use of additional metadata fields as it is common for
users to enrich their search by explicitly specifying authors or other metadata fields
(for further research on recommendations based on different types of metadata see the
work by Schaer et al. in these proceedings). A STR providing recommendations of
this kind could add additional benefits to a user’s search, especially if it recommends
e.g. the main authors of a specific discipline.</p>
          <p>Acknowledgements. This work was partly funded by DFG, grant no. SU 647/5-2.
References</p>
        </sec>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref id="ref1">
        <mixed-citation>
          1.
          <string-name>
            <surname>Furnas</surname>
            ,
            <given-names>G.W.</given-names>
          </string-name>
          , et al.:
          <article-title>The Vocabulary Problem in Human-System Communication</article-title>
          .
          <source>Communications of the ACM</source>
          <volume>30</volume>
          (
          <issue>11</issue>
          ),
          <fpage>964</fpage>
          -
          <lpage>971</lpage>
          (
          <year>1987</year>
          )
        </mixed-citation>
      </ref>
      <ref id="ref2">
        <mixed-citation>
          2.
          <string-name>
            <surname>Hargittai</surname>
          </string-name>
          , E.:
          <article-title>Hurdles to information seeking: Spelling and typographical mistakes during users' online behavior</article-title>
          .
          <source>Journal of the Association of Information Systems</source>
          <volume>7</volume>
          (
          <issue>1</issue>
          ),
          <fpage>52</fpage>
          -
          <lpage>67</lpage>
          (
          <year>2006</year>
          )
        </mixed-citation>
      </ref>
      <ref id="ref3">
        <mixed-citation>
          3.
          <string-name>
            <surname>Kluck</surname>
            ,
            <given-names>M.:</given-names>
          </string-name>
          <article-title>The GIRT Data in the Evaluation of CLIR Systems - from 1997 Until 2003</article-title>
          . In: Peters,
          <string-name>
            <given-names>C.</given-names>
            ,
            <surname>Gonzalo</surname>
          </string-name>
          ,
          <string-name>
            <given-names>J.</given-names>
            ,
            <surname>Braschler</surname>
          </string-name>
          ,
          <string-name>
            <given-names>M.</given-names>
            ,
            <surname>Kluck</surname>
          </string-name>
          ,
          <string-name>
            <surname>M.</surname>
          </string-name>
          , et al. (eds.)
          <article-title>CLEF 2003</article-title>
          .
          <article-title>LNCS</article-title>
          , vol.
          <volume>3237</volume>
          , pp.
          <fpage>376</fpage>
          -
          <lpage>390</lpage>
          . Springer, Heidelberg (
          <year>2004</year>
          )
        </mixed-citation>
      </ref>
      <ref id="ref4">
        <mixed-citation>
          4.
          <string-name>
            <surname>Mitra</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          , et al.:
          <article-title>Improving automatic query expansion</article-title>
          .
          <source>In: Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</source>
          , pp.
          <fpage>206</fpage>
          -
          <lpage>214</lpage>
          . ACM,
          <string-name>
            <surname>Melbourne</surname>
          </string-name>
          (
          <year>1998</year>
          )
        </mixed-citation>
      </ref>
      <ref id="ref5">
        <mixed-citation>
          5.
          <string-name>
            <surname>Mutschke</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          , et al.:
          <article-title>Science models as value-added services for scholarly information systems</article-title>
          .
          <source>Scientometrics</source>
          <volume>89</volume>
          (
          <issue>1</issue>
          ),
          <fpage>349</fpage>
          -
          <lpage>364</lpage>
          (
          <year>2011</year>
          )
        </mixed-citation>
      </ref>
      <ref id="ref6">
        <mixed-citation>
          6.
          <string-name>
            <surname>Petras</surname>
          </string-name>
          , V.:
          <article-title>How one word can make all the difference - using subject metadata for automatic query expansion and reformulation</article-title>
          .
          <source>In: Working Notes for the CLEF 2005 Workshop</source>
          , Vienna, Austria,
          <source>September</source>
          <volume>21</volume>
          -
          <fpage>23</fpage>
          (
          <year>2005</year>
          )
        </mixed-citation>
      </ref>
      <ref id="ref7">
        <mixed-citation>
          7.
          <string-name>
            <surname>Petras</surname>
          </string-name>
          , V.:
          <article-title>Translating Dialects in Search: Mapping between Specialized Languages of Discourse and Documentary Languages</article-title>
          . University of California (
          <year>2006</year>
          )
        </mixed-citation>
      </ref>
      <ref id="ref8">
        <mixed-citation>
          8.
          <string-name>
            <surname>Xu</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Croft</surname>
          </string-name>
          , W.B.:
          <article-title>Query expansion using local and global document analysis</article-title>
          .
          <source>In: Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</source>
          , pp.
          <fpage>4</fpage>
          -
          <lpage>11</lpage>
          . ACM,
          <string-name>
            <surname>Zurich</surname>
          </string-name>
          (
          <year>1996</year>
          )
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>