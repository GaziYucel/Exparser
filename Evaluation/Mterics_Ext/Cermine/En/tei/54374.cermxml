<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta>
      <journal-title-group>
        <journal-title>European Survey Research Association</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1864-3361</issn>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="doi">10.18148/srm/2017.v11i1.7149</article-id>
      <title-group>
        <article-title>Bias and e ciency loss in regression estimates due to duplicated observations: a Monte Carlo simulation</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>Luxembourg</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
          <xref ref-type="aff" rid="aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Keywords: Duplicate records</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
          <xref ref-type="aff" rid="aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Estimation Bias</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
          <xref ref-type="aff" rid="aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Monte Carlo Simulation</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
          <xref ref-type="aff" rid="aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Inference</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
          <xref ref-type="aff" rid="aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Survey Data Quality</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
          <xref ref-type="aff" rid="aff2">2</xref>
        </contrib>
        <aff id="aff0">
          <label>0</label>
          <institution>Contact information: Francesco Sarracino, STATEC</institution>
          ,
          <addr-line>13 rue</addr-line>
        </aff>
        <aff id="aff1">
          <label>1</label>
          <institution>LCSR National Research University Higher School of Economics Moscow</institution>
          ,
          <addr-line>Russian Federation</addr-line>
        </aff>
        <aff id="aff2">
          <label>2</label>
          <institution>Małgorzata Mikucka Mannheimer Zentrum für Europäische Sozialforschung (MZES) Mannheim, Germany and LCSR National Research University Higher School of Economics Moscow</institution>
          ,
          <addr-line>Russian Federation</addr-line>
        </aff>
      </contrib-group>
      <pub-date>
        <year>1864</year>
      </pub-date>
      <volume>11</volume>
      <issue>1</issue>
      <fpage>17</fpage>
      <lpage>44</lpage>
      <abstract>
        <p>Recent studies documented that survey data contain duplicate records. In this paper, we assess how duplicate records a ect regression estimates, and we evaluate the e ectiveness of solutions to deal with them. Results show that duplicates bias the estimated coe cients and standard errors. The chances of obtaining unbiased estimates when data contain 40 doublets (about 5% of the sample) range between 3.5% and 11.5% depending on the distribution of duplicates. If 7 quintuplets are present in the data (2% of the sample), then the probability of obtaining biased estimates ranges between 11% and 20%. Weighting the duplicate records by the inverse of their multiplicity, or dropping superfluous duplicates outperform other solutions in all considered scenarios in reducing the bias and the risk of obtaining biased estimates. However, both solutions overestimate standard errors, reducing the statistical power of estimates. Our study illustrates the risk of using data in presence of duplicate records and call for further research on strategies to analyse a ected data.</p>
      </abstract>
    </article-meta>
  </front>
  <body>
    <sec id="sec-1">
      <title>1 Introduction</title>
      <p>1.1</p>
    </sec>
    <sec id="sec-2">
      <title>Duplicate records in social survey data</title>
      <p>
        <xref ref-type="bibr" rid="ref16">Slomczynski et al. (2017)</xref>
        analyzed 1,721 national surveys
belonging to 22 comparative survey projects, with data
coming from 142 countries and nearly 2.3 million respondents.
The analysis identified 5,893 duplicate records in 162
national surveys from 17 projects coming from 80 countries.
The duplicate records were unequally distributed across the
surveys. For example, they appeared in 19.6% of surveys of
the World Values Survey (waves 1–5) and in 3.4% of surveys
of the European Social Survey (waves 1–6). Across survey
projects, di erent numbers of countries were a ected.
Latinobarometro is an extreme case where surveys from 13 out
of 19 countries contained duplicate records. In the Americas
Barometer 10 out of 24 countries were a ected, and in the
International Social Survey Programme 19 out of 53 countries
contained duplicate records.
      </p>
      <p>
        Even though the share of duplicate records in most
surveys did not exceed 1%, in some of the national surveys it
was high, exceeding 10% of the sample. In 52% of the
affected surveys
        <xref ref-type="bibr" rid="ref16">Slomczynski et al. (2017)</xref>
        found only a
single pair of duplicate records. However, in 48% of surveys
containing duplicates they found various patterns of
duplicate records, such as multiple doublets (i.e. multiple pairs
of identical records) or identical records repeated three, four,
or more times. For instance, the authors identified 733
duplicate records (60% of the sample), including 272 doublets
and 63 triplets in the Ecuadorian sample of Latinobarometro
collected in the year 2000. Another example are data from
Norway registered by the International Social Survey
Programme in 2009, where 54 duplicate records consisted of
27 doublets, 36 duplicate records consisted of 12 triplets,
24 consisted of 6 quadruplets, 25 consisted of 5 quintuplets;
along with, one sextuplet, one septuplet, and one octuplet
(overall 160 duplicate records, i.e. 11.0% of the sample).
      </p>
      <p>
        These figures refer to full duplicates. However, other
research analyzed the prevalence of near duplicates, that is
records which di er for only a small number of variables.
        <xref ref-type="bibr" rid="ref11">Kuriakose and Robbins (2016)</xref>
        analyzed near duplicates in
data sets commonly used in social sciences and showed that
16% of analysed surveys reported a high risk of widespread
falsification with near duplicates. The authors emphasized
that demographic and geographical variables are rarely
falsified, because they usually have to meet the sampling frame.
Behavioral and attitudinal variables, on the other hand, were
falsified more often. In such cases, interviewers may only
copy selected sequences of answers from other respondents,
so that the correlations between variables are as expected,
and the forgery remains undetected.
      </p>
    </sec>
    <sec id="sec-3">
      <title>1.2 Implications for estimation results</title>
      <p>
        Duplicate records may a ect statistical inference in
various ways. If duplicate records introduce “random noise”,
then they may produce an attenuation bias, i.e. bias the
estimated coe cient towards zero
        <xref ref-type="bibr" rid="ref6">(Finn &amp; Ranchhod, 2013)</xref>
        .
However, if the duplicate records do not introduce random
noise, they may bias the estimated correlations in other
directions. The size of the bias should increase with the number
of duplicate interviews, and it should depend on the di
erence of covariances and averages between the original and
duplicate interviews (Schräpler &amp; Wagner, 2005).
      </p>
      <p>
        On the other hand, duplicate records can reduce the
variance, and thus they may artificially increase the statistical
power of estimation techniques. The result is the opposite of
the attenuation bias: narrower estimated confidence intervals
and stronger estimated relationships among variables
        <xref ref-type="bibr" rid="ref11">(Kuriakose &amp; Robbins, 2016)</xref>
        . In turn, this may increase the
statistical significance of the coe cients and a ect the substantive
conclusions.
      </p>
      <p>
        The implications of duplicates for regression estimates
may di er according to the characteristics of the
observations being duplicated.
        <xref ref-type="bibr" rid="ref16">Slomczynski et al. (2017)</xref>
        suggested
that “typical” cases, i.e. the duplicate records located near
the median of a variable, may a ect estimates less than
“deviant” cases, i.e. duplicate records located close to the ties of
the distribution.
      </p>
      <p>
        The literature on how duplicate records a ect estimates
from regression analysis, and how to deal with them is
virtually not existing. Past studies focused mainly on strategies to
identify duplicate and near-duplicate records
        <xref ref-type="bibr" rid="ref11 ref15 ref3 ref8">(Elmagarmid,
Ipeirotis, &amp; Verykios, 2007; Hassanzadeh &amp; Miller, 2009;
Kuriakose &amp; Robbins, 2016; Schreiner, Pennie, &amp;
Newbrough, 1988)</xref>
        . However some studies analyzed how
intentionally falsified interviews (other than duplicates) a ected
summary statistics and estimation results.
        <xref ref-type="bibr" rid="ref13">Schnell (1991)</xref>
        studied the consequences of including purposefully falsified
interviews in the 1988 German General Social Survey
(ALLBUS). The results showed a negligible impact on the mean
and standard deviation of variables. However, the falsified
responses produced stronger correlations between objective
and subjective measures, more consistent scales (with higher
Cronbach’s ), higher R2, and more significant predictors
in OLS regression. More recently,
        <xref ref-type="bibr" rid="ref14">Schräpler and Wagner
(2005)</xref>
        and
        <xref ref-type="bibr" rid="ref6">Finn and Ranchhod (2013)</xref>
        did not confirm the
greater consistency of falsified data. On the contrary, they
showed a negligible e ect of falsified interviews on
estimation bias and e ciency.
1.3
      </p>
    </sec>
    <sec id="sec-4">
      <title>Current analysis</title>
      <p>
        Our study is the first analysis of how duplicate records
a ect the bias and e ciency of regression estimates. We
focus on two research questions: first, how do duplicates a ect
regression estimates? Second, how e ective are the possible
solutions? We use a Monte Carlo simulation, a technique
for generating random samples on a computer to study the
consequences of probabilistic events
        <xref ref-type="bibr" rid="ref5 ref7">(Ferrarini, 2011;
Fishman, 2005)</xref>
        . In our simulations we consider three scenarios
of duplicate data:
Scenario 1. when one record is multiplied several times (a
sextuplet, an octuplet, and a decuplet),
Scenario 2. when several records are duplicated once (16,
40, and 79 doublets, which correspond to 2%, 5% and
10% of the sample respectively),
Scenario 3. when several records are duplicated four times
(7, 16, and 31 quintuplets, which correspond to 2%,
5% and 10% of the sample).
      </p>
      <p>
        We chose the number of duplicates to mimic the results
provided by
        <xref ref-type="bibr" rid="ref16">Slomczynski et al. (2017)</xref>
        . We also investigate how
regression estimates change when duplicates are located in
specific parts of the distribution of the dependent variable.
We evaluate four variants, namely:
Variant i. when the duplicate records are chosen randomly
from the whole distribution of the dependent variable
(we label this variant “unconstrained” as we do not
impose any limitation on where the duplicate records are
located);
Variant ii. when they are chosen randomly between the first
and third quartile of the dependent variable (i.e. when
they are located around the median: this is the
“typical” variant);
Variant iii. when they are chosen randomly below the first
quartile of the dependent variable (this is the first
“deviant” variant);
Variant iv. when they are chosen randomly above the third
quartile of the dependent variable (this is the second
“deviant” variant).
      </p>
      <p>
        We expect, consistently with the suggestion by
        <xref ref-type="bibr" rid="ref16">Slomczynski
et al. (2017)</xref>
        , that Variants iii and iv a ect regression
estimates more than Variant i, and that Variant ii a ects them
the least. Additionally, we repeat the whole analysis to test
the robustness of our findings by checking how the position
on the distribution of one of the independent variables a ects
regression estimates.
      </p>
      <p>For each scenario and variant we compute the
following measures to assess how duplicates a ect regression
estimates:
Measure A. percentage bias of coe cients;
Measure B. bias of the standard errors;
Measure C. risk of obtaining biased estimates, as measured
by Dfbetas;
Measure D. Root Mean Square Error (RMSE), which
informs about the e ciency of the estimates.</p>
      <p>We consider five solutions to deal with duplicate records,
and we assess their ability to reduce the bias and the e
ciency loss:
Solution a. “naive” estimation, i.e. analysing the data as if
they were correct;
Solution b. dropping all the duplicates from the data;
Solution c. flagging the duplicate records and including the
flag among the predictors;
Solution d. dropping all superfluous duplicates;
Solution e. weighting the duplicate records by the inverse of
their multiplicity.</p>
      <p>Finally, we check the sensitivity of our results to the
sample size. Our basic analysis uses a sample of N = 1; 500,
because many nationally representative surveys provide
samples of similar sizes. However, we also run the simulation for
samples of N = 500 and N = 5; 000 to check the robustness
of our results to the chosen sample sizes.</p>
      <p>Table 1
Matrix of correlations used to generate the original data set.
variables
x
z
t
x
1:00
0:04
0:09
z
1:00
0:06</p>
      <p>t
1:00
2</p>
    </sec>
    <sec id="sec-5">
      <title>Method</title>
      <p>
        To assess how duplicate records a ect the results of OLS
regression we use a Monte Carlo simulation
        <xref ref-type="bibr" rid="ref5 ref7">(Ferrarini, 2011;
Fishman, 2005)</xref>
        . The reason is that we need an artificial data
set where the relationships among variables are known, and
in which we iteratively manipulate the number and
distribution of duplicates. The random element in our simulation
is the choice of records to be duplicated, and the choice of
observations which are replaced by duplicates. At each
iteration, we compare the regression coe cients in presence of
duplicates with the true coe cients (derived from data
without duplicates) to tell whether duplicates a ect regression
estimates.
      </p>
      <p>Our analysis consists of four steps. First, we generate the
initial data set. Second, we duplicate randomly selected
observations according to the three scenarios and four variants
mentioned above. In the third step we estimate regression
models using a “naive” approach, i.e. treating data with
duplicates as if they were correct (Solution a). In the same step
we also estimate regression models using the four alternative
solutions (b–e) to deal with duplicate records. Finally, we
compute the bias of coe cients and standard errors, the risk
of obtaining biased estimates, and the Root Mean Square
Error to assess the e ect of duplicates on regression estimates
and the e ectiveness of the solutions. Figure 1 summarizes
our strategy.
2.1</p>
    </sec>
    <sec id="sec-6">
      <title>Data generation</title>
      <p>
        We begin by generating a data set of N = 1; 500
observations which contains three variables: x, z, and t. We create
the original data set using random normally distributed
variables with a known correlation matrix (shown in Table 1).
The correlation matrix is meant to mimic real survey data
and it is based on the correlation of household income, age,
and number of hours worked as retrieved from the sixth wave
of the
        <xref ref-type="bibr" rid="ref4">European Social Survey (2015</xref>
        ).
      </p>
      <p>We generate the dependent variable (y) as a linear function
of x, z, and t as reported in Equation 1:
yi = 5:36
0:04 xi + 0:16 zi + 0:023 ti + i
(1)
where the coe cients are also retrieved from the sixth wave
of the European Social Survey. All variables and the error
term i are normally distributed. The descriptive statistics</p>
    </sec>
    <sec id="sec-7">
      <title>Data generation:</title>
    </sec>
    <sec id="sec-8">
      <title>Scenarios:</title>
    </sec>
    <sec id="sec-9">
      <title>Variants:</title>
    </sec>
    <sec id="sec-10">
      <title>Solutions:</title>
      <p>(2,500 replications)
one
original
data set,
(N=1500)
1.
1 observation duplicated
5, 7, and 9 times
(sextuplet, octuplet, decuplet)
2.
16, 40, and 79
observations duplicated
1 time (doublets)
3.
7, 16, and 31
observations duplicated 4
times (quintuplets)
i.</p>
      <p>Unconstrained:
randomly drawn from the
overall distribution
ii.</p>
      <p>Typical:
randomly drawn from
around the median
iii.</p>
      <p>Deviant: randomly
drawn from the
upper quartile
iv.</p>
      <p>Deviant: randomly
drawn from the
lower quartile
a.
“naive”
estimation
b.
excluding all
duplicates
c.
duplicates
flagged and
controlled for
d.
excluding
superflous
duplicates
e.
weighted by
the inverse of
multiplicities
(1 regression model per replication)</p>
    </sec>
    <sec id="sec-11">
      <title>Measures of bias and e ciency:</title>
      <p>A.</p>
      <p>Bias of
coe cients</p>
      <p>B.</p>
      <p>Bias of
standard errors
Figure 1. Diagram summarizing the empirical strategy.
of the generated data set are shown in the first four lines of
Table A1 in Appendix A.
2.2</p>
    </sec>
    <sec id="sec-12">
      <title>Duplicating selected observations</title>
      <p>In the second step we use a Monte Carlo simulation to
generate duplicate records, which replace for randomly
chosen original records, i.e. the interviews that would have been
conducted if no duplicates had been introduced in the data.
This strategy is motivated by the assumption that duplicate
records substitute for authentic interviews. Thus, if
duplicates are present, researchers do not only face the risk of
fake or erroneous information, but they also lose
informaC.</p>
      <p>Risk of
obtaining biased
estimates</p>
      <p>D.</p>
      <p>RMSE
tion from genuine respondents. We duplicate selected
observations in three scenarios (each comprising three cases)
and in four variants. Thus, overall we investigate 36 patterns
(3 3 4 = 36) of duplicate records. For each pattern we run
2,500 replications.</p>
      <p>
        Scenario 1: a sextuplet, an octuplet, and a decuplet.
In the first scenario we duplicate one randomly chosen record
5, 7, and 9 times, thus introducing in the data a sextuplet,
an octuplet, and a decuplet of identical observations which
replace for 5, 7, and 9 randomly chosen original
observations. These cases are possible in the light of the analysis by
        <xref ref-type="bibr" rid="ref16">Slomczynski et al. (2017)</xref>
        who identified in real survey data
      </p>
      <sec id="sec-12-1">
        <title>Variant i: unconstrained</title>
        <p>instances of octuplets. In this scenario the share of duplicates
in the sample is small, ranging from 0,4% for a sextuplet to
0,7% for a decuplet.</p>
        <p>
          Scenario 2: 16, 40, and 79 doublets. In the second
scenario we duplicate sets of 16, 40, and 79 randomly chosen
observations one time, creating 16, 40, and 79 pairs of
identical observations (doublets). In this scenario the share of
duplicates is 2,1% (16 doublets), 5,3% (40 doublets), and
10,5% (79 doublets). These shares are consistent with the
results by
          <xref ref-type="bibr" rid="ref16">Slomczynski et al. (2017)</xref>
          , as in their analysis
about 15% of the a ected surveys had 10% or more duplicate
records.
        </p>
        <p>Scenario 3: 7, 16, and 31 quintuplets. In the third
scenario we duplicate sets of 7, 16 and 31 randomly chosen
observations 4 times, creating 7, 16 and 31 quintuplets. They
replace, for 28, 64, and 124 randomly chosen original records
respectively. In this scenario the share of duplicate records is
2,3% (7 quintuplets), 5,3% (16 quintuplets), and 10,3% (31
quintuplets).</p>
        <p>To check whether the position of duplicates in the
distribution matters, we run each of the scenarios in four variants,
as presented in Figure 2.</p>
        <p>Variant i (“unconstrained”). The duplicates and the
replaced interviews are randomly drawn from the overall
distribution of the dependent variable.</p>
        <p>Variant ii (“typical”). The duplicates are randomly
drawn from the values around the median of the dependent
variable, i.e. between the first and third quartile, and the
replaced interviews are drawn from the overall distribution.</p>
        <p>Variant iii and iv (“deviant”). In Variant iii the
duplicates are randomly drawn from the lower quartile of the
dependent variable; in Variant iv they are randomly drawn from
the upper quartile of the dependent variable. The replaced
interviews are drawn from the overall distribution.</p>
        <p>To illustrate our data, Table A1 in Appendix A reports the
descriptive statistics of some of the data sets produced during
the replications (lines 5 to 45).
2.3</p>
        <p>“Naive” estimation and alternative solutions</p>
        <p>In the third step we run a “naive” estimation which takes
data as they are, and subsequently we investigate the four
solutions to deal with duplicates. For each solution we estimate
the following model:
yi =
+ x xi + z zi + t ti + "i
(2)</p>
        <p>Solution a: “naive” estimation. First, we investigate
what happens when researchers neglect the presence of
duplicate observations. In other words, we analyze data with
duplicate records as if they were correct. This allows us to
estimate the percentage bias, the standard errors, the risk of
obtaining biased estimates, and the Root Mean Square Error
resulting from the mere presence of duplicate records (see
Section 2.4).
fY (y)
fY (y)
fY (y)
fY (y)</p>
      </sec>
      <sec id="sec-12-2">
        <title>Variant ii: “typical”</title>
      </sec>
      <sec id="sec-12-3">
        <title>Variant iii: “deviant”</title>
        <p>y
y
y
y</p>
      </sec>
      <sec id="sec-12-4">
        <title>Variant iv: “deviant”</title>
        <p>Figure 2. Presentation of the Variants i–iv used in the Monte
Carlo simulation.</p>
        <p>Solution b: Drop all duplicates. In Solution b we drop
all duplicates, including the observations that may come
from true interviews. We consider such a case, because, if
records are identical on some, but not on all variables (most
likely, di erences may exist on demographic and
geographical variables to reflect the sampling scheme), then it is not
obvious to tell the original observations from the fake
duplicates. It is also possible that all duplicates are forged and
should be excluded from the data. Therefore, rather that
deleting the superfluous duplicates and retaining the original
records, we exclude all duplicate records from the data at the
cost of reducing the sample size.</p>
        <p>
          Solution c: Flag duplicated observations and control
for them. This solution is similar to the previous one
because we identify all duplicate records as suspicious.
However, rather than dropping them, we generate a dichotomous
variable (duplicate = 1, otherwise = 0), and include it among
the predictors in Equation 2.
          <xref ref-type="bibr" rid="ref16">Slomczynski et al. (2017)</xref>
          proposed this solution as a way to control for the error generated
by duplicate records.
        </p>
        <p>
          Solution d: Drop superfluous duplicates.
“[E]liminating duplicate and near duplicate observations
from analysis is imperative to ensuring valid inferences”
          <xref ref-type="bibr" rid="ref11">(Kuriakose &amp; Robbins, 2016, p. 2)</xref>
          . Hence, we delete
superfluous duplicates to retain a sample of unique records.
The di erence compared to Solution b is that we keep one
record for each set of duplicates.
        </p>
        <p>
          Solution e: Weight by the inverse of multiplicities.
          <xref ref-type="bibr" rid="ref12">Lessler and Kalsbeek (1992)</xref>
          proposed this method. We
construct a weight which takes the value of 1 for unique
records, and the value of the inverse of multiplicity for
duplicate records. For example, the weight takes the value 0.5
for doublets, 0.2 for quintuplets, 0.1 for decuplets, etc.
Subsequently, we use these weights to estimate Equation 2.
2.4
        </p>
      </sec>
    </sec>
    <sec id="sec-13">
      <title>The assessment of bias and e ciency</title>
      <p>We use four measures to assess the consequences of
duplicates for regression estimates, and to investigate the e
ciency of the solutions to deal with them.</p>
      <p>Measure A: Bias of coe cients. This macro measure
of bias informs whether a coe cient is systematically over
or under estimated. It is computed as follows:</p>
      <p>Bias of coe cients = BBBBB0 bi</p>
      <p>B
B
@
1
C
CCC 100%
C
C
C
A
(3)
where i indicates a specific replication,
is the true
coefficient from Equation 1, and bi is the average of estimated
coe cients (bi).</p>
      <p>Measure B: Bias of standard errors. To test whether
duplicates artificially increase the power of regression
estimates, we compute the average of the standard errors
(SE(bi)) for each scenario, variant, and solution. For ease
of interpretation, we express our measure as a percentage of
the standard errors estimated in the true model (see Equation
4).</p>
      <p>Bias of S.E. = BBBB0 SE(bi) 1C</p>
      <p>CCC 100%</p>
      <p>B@B SE( ) CCA</p>
      <p>Measure C: Risk of obtaining biased estimates. It is
possible to obtain biased estimates even if the average bias
is zero. This can happen if the upward and downward biases
o set each other. To assess the risk of obtaining biased
estimates in a specific replication, we resort to Dfbetas, which
are normalized measures of how much specific observations
(in our case the duplicates) a ect the estimates of regression
coe cients. Dfbetas are defined as the di erence between
the estimated and the true coe cients, expressed in relation
to the standard error of the estimated coe cient (see
Equation 5).</p>
      <p>8
xi = &lt;&gt;&gt;1 if jDfbetaij &gt; 0:5;</p>
      <p>&gt;&gt;:0 otherwise.</p>
      <p>Pr(Bias) = xi 100%
(4)
(5)
(6)
(7)
Dfbetai = b</p>
      <p>SE(bi)
i
Dfbetas measure the bias of a specific estimation, thus, they
complement percentage bias by informing about the risk of
obtaining biased estimates. The risk is computed according
to Equation 6. We set the cuto value to 0.5, i.e. we consider
the estimation as biased if the coe cients di er from the true
values by more than half standard deviation.1</p>
      <p>Measure D: Root Mean Square Error (RMSE). Root
mean square error is an overall measure of the quality of the
prediction and it reflects both its bias and its dispersion (see
Equation 7).</p>
      <p>RMSE =
s
i
b
2
+ SE(bi) 2</p>
      <p>The RMSE is expressed in the same units of the variables
(in this case the coe cients), and it has no clear-cut
threshold value. For ease of interpretation we express RMSE as
a percentage of the respective coe cients from Equation 1,
thus we report the normalized RMSE.</p>
      <p>1This cuto value is more conservative than the customary
assumed value of p2N , which, for N = 1; 500 leads to the threshold
value of 0.05.</p>
    </sec>
    <sec id="sec-14">
      <title>Results 3.1</title>
    </sec>
    <sec id="sec-15">
      <title>Percentage bias</title>
      <p>Table 2 shows the percentage bias of the coe cient x for
the considered scenarios, variants, and solutions. The results
for the other coe cients are reported in Tables B1–B3 in
Appendix B. The third column of Table 2 contains information
about the percentage bias for solution a, i.e. the “naive”
estimation.</p>
      <p>Overall, the percentage bias takes values between nearly
zero (in Scenario 1 and in all Scenarios in Variant i) and about
7%. For x it reaches the maximum values of 4% for 79
doublets and 6% for 31 quintuplets. The maximum bias for z
and t is 5%–7%, and for the intercept it is 2.5%–4% (see
Appendix B).</p>
      <p>The results show some regularities. First, number and
composition of duplicates matter. The bias systematically
increases with the share of duplicates in the data, and
duplicates consisting of quintuplets produce greater bias than
duplicates consisting of doublets. Second, the choice of records
to be duplicated plays a role. The “unconstrained” variant
(Variant i), where duplicate cases are randomly selected,
produces virtually no bias, even when the share of duplicates
reaches 10% of the sample. On the other hand, Variant ii
produces similar bias as Variants iii and iv. In other words,
contrary to our expectations, the duplication of “typical” records
produces a bias similar to the one induced by the presence
of “deviant” duplicates. Only randomly chosen duplicates
generate no bias. Third, although previous studies suggested
that duplicates may introduce “random noise” to the data,
thus leading to attenuation bias, we did not find the evidence
to support this expectation: depending on the Variant (ii–iv),
for each variable the presence of duplicates induces a mix of
overestimated and underestimated coe cients.</p>
      <p>Among the four solutions to deal with duplicates,
solutions d and e, i.e. dropping the superfluous duplicates and
weighting by the inverse of multiplicity perform the best in
all Variants, reducing the bias to zero. On the other hand,
dropping all duplicates (solution b) and flagging duplicates
and controlling for them in the regression (solution c)
perform poorly. Especially in Scenario 2 both these solutions
increase the bias of all coe cients; in Scenario 3 they reduce
the bias, but to a lesser degree than solutions d and e.</p>
      <p>In sum, duplicates can systematically bias regression
estimates if they are not randomly created. However, the bias
in our simulation did not exceed 10% of the true coe cients.
Moreover, dropping superfluous duplicates or weighting by
the inverse of their multiplicity are e ective ways to reduce
the bias to zero.
3.2</p>
    </sec>
    <sec id="sec-16">
      <title>Standard errors</title>
      <p>To understand whether duplicates artificially increase the
statistical power of regression estimates, we inspect the
average estimated standard errors, as shown in Table 3 for x.
The results for other coe cients are presented in Tables C1–
C3 in Appendix C.</p>
      <p>The results show, similarly to the case of percentage bias,
that duplicates in Variant i, i.e. randomly drawn from the
overall distribution (“unconstrained”), do not a ect the
estimates of the standard errors. In Variant ii, in which the
duplicates are located around the median of the dependent
variable, the estimated standard errors are biased downwards
by maximum 2%–3%, thus the confidence intervals are
narrower than in the true model. On the contrary, in Variants
iii and iv, i.e. the two “deviant” cases, duplicates lead to
standard errors biased upwards by maximum 2%–3%, and to
broader confidence intervals. Both e ects are stronger when
data contain more duplicates, i.e. when data contain 79
doublets or 31 quintuplets.</p>
      <p>Among the considered solutions, flagging and controlling
for duplicates (Solution c) leads to systematically narrower
confidence intervals. This is especially worrisome because
the same solution produces the most biased coe cients. In
other words, this solution may result in biased and significant
coe cients, thus a ecting the interpretation of the results.
The remaining three Solutions, b, d, and e, produce slightly
greater standard errors than the naive estimation. The
relative performance of the solutions varies across specific
coefficients: for x and z Solution e works better than dropping
duplicates, but it overestimates the standard errors more than
dropping the duplicates for t and the intercept.</p>
      <p>Summing up, we find no evidence that the duplicates
artificially increase the statistical power of the estimates if the
duplicates are created randomly. However, if duplicates are
chosen from the center of the distribution they may lead to
narrower confidence intervals, thus artificially increasing the
statistical power. On the other hand, duplication of “deviant”
cases reduces the power of estimates. In both cases the e ect
is small, up to 3% of the true standard errors. Yet, the most
e ective solutions to reduce the bias of coe cients, i.e.
dropping the superfluous duplicates and weighting by the inverse
of multiplicity, increase the estimated standard errors, thus
reducing the power of estimates.
3.3</p>
    </sec>
    <sec id="sec-17">
      <title>Risk of obtaining biased estimates</title>
      <p>While percentage bias informs about the average bias due
to duplicates, it is plausible that estimates in specific
replications have upward and downward biases which, on
average, o set each other. In other words, even with moderate
bias, researchers can obtain biased estimates in specific
estimations. To address this issue we turn to the analysis of
Dfbetas.</p>
      <p>Figure 3 shows box and whiskers diagrams of Dfbetas in
Scenarios 2 (upper panel) and 3 (lower panel) for Variant i,
i.e. when the duplicates are randomly drawn from the
overall distribution of the dependent variable. We do not report
Table 2</p>
      <sec id="sec-17-1">
        <title>Percentage bias of the</title>
        <p>x coe
cient (as a percentage of
x).</p>
        <p>Solution</p>
      </sec>
      <sec id="sec-17-2">
        <title>Scenario 3</title>
        <p>Variant i: “unconstrained”
7 quintuplets
16 quintuplets
31 quintuplets
Variant ii: “typical”
7 quintuplets
16 quintuplets
31 quintuplets
Variant iii: “deviant”
7 quintuplets
16 quintuplets
31 quintuplets
Variant iv: “deviant”
7 quintuplets
16 quintuplets
31 quintuplets
Table 3
Average standard error of x coe cient (expressed as a percentage of the true
standard error of x).</p>
        <p>Solution</p>
      </sec>
      <sec id="sec-17-3">
        <title>Scenario 1</title>
        <p>Variant i: “unconstrained”
1 sextuplet 100:0
1 octuplet 100:0
1 decuplet 100:0
Variant ii: “typical”
1 sextuplet 99:9
1 octuplet 99:8
1 decuplet 99:8
Variant iii: “deviant”
1 sextuplet 100:1
1 octuplet 100:2
1 decuplet 100:2
Variant iv: “deviant”
1 sextuplet 100:1
1 octuplet 100:2
1 decuplet 100:2</p>
      </sec>
      <sec id="sec-17-4">
        <title>Scenario 2</title>
        <p>Variant i: “unconstrained”
16 doublets 100:0
40 doublets 100:0
79 doublets 100:0
Variant ii: “typical”
16 doublets 99:6
40 doublets 99:0
79 doublets 98:0
Variant iii: “deviant”
16 doublets 100:4
40 doublets 100:9
79 doublets 101:7
Variant iv: “deviant”
16 doublets 100:4
40 doublets 100:9
79 doublets 101:8</p>
      </sec>
      <sec id="sec-17-5">
        <title>Scenario 3</title>
        <p>Variant i: “unconstrained”
7 quintuplets 100:0
16 quintuplets 100:0
31 quintuplets 100:0
Variant ii: “typical”
7 quintuplets 99:3
16 quintuplets 98:5
31 quintuplets 97:0
Variant iii: “deviant”
7 quintuplets 100:6
16 quintuplets 101:3
31 quintuplets 102:4
Variant iv: “deviant”
7 quintuplets 100:7
16 quintuplets 101:4
31 quintuplets 102:5
results for Scenario 1 because in this case the risk of
obtaining biased estimates is virtually zero. Results, however, are
detailed in Table 4. On the y-axis we report the Dfbetas, on
the x-axis we report the coe cients and the solutions. The
two horizontal solid lines identify the cuto values of
Dfbetas (0.5) separating the replications with acceptable bias from
the unacceptable ones. The diagrams show that the range of
Dfbetas increases with the share of duplicates in the data,
and it is larger for quintuplets (Scenario 3) than for doublets
(Scenario 2).</p>
        <p>The average probability of obtaining unbiased estimates
for all coe cients is shown in Table 4. Column a shows that,
in case of “naive” estimations, the risk of biased estimates
varies from 0,14% (Scenario 1, one sextuplet, Variant ii) to
about 58% (Scenario 3, 31 quintuplets, Variants iii and iv).
In Scenario 1 the risk is small, with 89%–99% probability of
obtaining unbiased estimates.</p>
        <p>The results show three regularities. First, the risk of
obtaining biased estimates increases with the share of
duplicates: it is 0.2%–0.7% (depending on the variant) for 16
doublets, but it grows to 14.0%–32.5% when 79 doublets
are included in the data. In Scenario 3 it grows from 3.0%–
20.2% for 7 quintuplets, to 43.0%–58.6% when data contain
31 quintuplets.</p>
        <p>Second, when the duplicates constitute the same share of
the data, the risk of obtaining biased estimates is higher for
quintuplets than for doublets. For example, when duplicates
constitute 2% of the sample, the risk of obtaining biased
estimates is below 1% if they are 16 doublets, but ranges
between 3% and 20% (depending on the variant) for 7
quintuplets. When duplicates constitute 10% of the data, the risk
of obtaining biased estimates is 14%–32% in case of 79
doublets, but 43%–58% for 31 quintuplets.</p>
        <p>Third, the risk of obtaining biased estimates is the highest
in Variants iii and iv, i.e. when the duplicates are located
on the ties, and lowest in Variant ii, when the duplicates are
located around the median. For example, with 7 quintuplets,
the probability of obtaining biased estimates is about 3% in
Variant ii, but rises to about 20% in Variants iii and iv. For 31
quintuplets the risk is about 43% in Variant ii, but over 58%
in Variants iii and iv.</p>
        <p>As in case of percentage bias, weighting by the inverse
of the multiplicity (Solution e), and dropping the superfluous
duplicates (Solution d) perform better than other solutions
(see Table 4 and Figure 3). In Scenario 2, when doublets
constitute about 5% of the data, these two solutions reduce the
probability of obtaining biased estimates from about 4% (in
Variants i and ii) or about 11% (Variants iii and iv) to under
1% in all cases. In case of 79 doublets, i.e when duplicates
constitute about 10% of the sample, the risk of obtaining
biased estimates reduces to about 3%, independently from the
location of the duplicates, whereas it ranges between 14%
and 33% for the naive estimation. In Scenario 3, when
quintuplets constitute about 5% of the data, the risk of obtaining
biased estimates declines from 19%–43% (depending on the
variant) to about 2%. When quintuplets constitute about 10%
of the sample, solutions d and e decrease the risk of obtaining
biased estimates from 43%–59% to about 9%.</p>
        <p>To sum up, weighting by the inverse of multiplicity and
dropping the superfluous duplicates are the most e ective
solutions among the examined ones. Moreover, they perform
particularly well when the duplicates are located on the ties
of the distribution, i.e. when the risk of bias is the highest.</p>
        <p>On the other hand, Solutions b (excluding all duplicates)
and c (flagging the duplicates) perform worse. Flagging
duplicates and controlling for them (c) fails to reduce the risk
of obtaining biased estimates in both Scenarios 2 and 3.
Excluding all duplicates (b) reduces the risk of obtaining biased
estimates in Scenario 3 (quintuplets), but it performs poorly
in Scenario 2: if the doublets are located on the ties, then
dropping all duplicate records decreases the probability of
obtaining unbiased estimates.
3.4</p>
      </sec>
    </sec>
    <sec id="sec-18">
      <title>Root Mean Square Error</title>
      <p>Table 5 shows the values of normalized RMSE for coe
cient x. Results for other coe cients are available in Tables
D1–D3 in Appendix D. Scores are overall small, reaching
about 9% of the x coe cient, 15% of z, 24% of t, and 6%
of the intercept.</p>
      <p>The RMSE captures both the bias and the standard errors,
thus it is not surprising that the results are consistent with
those presented in the sections above. First, RMSE increases
with the number of duplicates, and it is the highest for 79
doublets and 31 quintuplets. Second, the presence of
randomly duplicated observations (Variant i) has little e ect on
the e ciency of the estimates, whereas the presence of
“typical” (Variant ii) and “deviant” (Variant iii and iv) duplicates
reduces the e ciency of estimates.</p>
      <p>Consistently with previous results, Solutions d and e, i.e.
dropping superfluous duplicates and weighting the data
perform the best, reasonably reducing the RMSE values. In
contrast to that, flagging the duplicates and controlling for them
(Solution c) performs poorly, and in some cases (especially
in Scenario 2, but for x also in Scenario 3) it further reduces
the e ciency of the estimates.
3.5</p>
    </sec>
    <sec id="sec-19">
      <title>Robustness</title>
      <p>Varying sample size. By setting up our experiment, we
arbitrarily chose a sample size of N = 1; 500 observations
to mimic the average size of many of the publicly available
social surveys. To check whether our results are independent
from our choice, we repeated the experiment using two
alternative samples: N = 500 and N = 5; 000. In Figure 4
we report the results (DFbetas) for Scenario 2, Variant i. The
complete set of results is available upon request.
2
1
1
−
2
−
4
2
2
−
s
tea 0
b
−
f
d
s
a
t
e
b
−
fd 0
a b c d e a b c d e a b c d e a b c d e
_cons t x z
7 quintuplets (2%)
16 quintuplets (5%)
31 quintuplets (10%)
16 doublets (2%)
40 doublets (5%)</p>
      <p>79 doublets (10%)
a b c d e a b c d e a b c d e a b c d e
_cons t x z
a b c d e a b c d e a b c d e a b c d e
_cons t x z
a b c d e a b c d e a b c d e a b c d e
_cons t x z
Figure 3. Box and whiskers diagrams of Dfbetas in Scenario 2 and 3, Variant i. The duplicate records are randomly drawn
from the overall distribution. Box and whiskers show the distribution of Dfbetas (across 2,500 replications) for each of the
coe cients in the model and for the solutions a to e.</p>
      <p>Notes: a: “Naive” estimation; b: Drop all duplicates; c: Flag and control; d: Drop superfluous duplicates; e: Weighted
regression. _cons: regression constant; x: x; z: z; t: t.
Table 4
Probability of obtaining unbiased estimates (Dfbetai &lt; 0:5).</p>
      <p>Solution
89:2
71:9
55:7
97:0
80:7
57:1
80:3
58:2
41:4
79:9
57:1
41:9
Table 5</p>
      <sec id="sec-19-1">
        <title>Normalized RMSE of the</title>
        <p>x coe
cient (in percentage).</p>
        <p>Solution
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:2
9:8
9:2
9:5
10:3
9:2
9:3
9:7
9:1
9:1
9:1
9:2
9:6
10:9
9:3
9:8
11:2
9:2
9:5
10:3
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:2
9:4
9:6
9:3
9:7
10:7
9:2
9:6
10:9
9:2
9:4
10:2
9:2
9:3
9:6
9:2
9:4
9:8
9:2
9:4
9:7
9:2
9:3
9:6
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:2
9:8
9:4
11:4
16:8
9:3
10:5
14:3
9:1
9:1
9:1
9:1
9:5
10:8
9:3
10:5
13:9
9:2
9:9
12:4
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:2
9:3
9:1
9:2
9:3
9:1
9:2
9:4
9:1
9:2
9:4
9:2
9:3
9:5
9:2
9:3
9:5
9:2
9:3
9:5
9:2
9:3
9:5
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:2
9:1
9:2
9:3
9:1
9:1
9:1
9:1
9:1
9:1
9:1
9:2
9:4
9:2
9:3
9:4
9:1
9:2
9:4
9:1
9:2
9:3
s
tea 0
b
−
f
d
s
tea 0
b
−
f
d
2
1
1
−
2
−
2
1
1
−
2
−
a b c d e a b c d e a b c d e a b c d e
_cons t x z
50 doublets (2%)
130 doublets (5%)
270 doublets (10%)
5 doublets (2%)
13 doublets (5%)
27 doublets (10%)
a b c d e a b c d e a b c d e a b c d e
_cons t x z
a b c d e a b c d e a b c d e a b c d e
_cons t x z
a b c d e a b c d e a b c d e a b c d e
_cons t x z
Figure 4. Box and whiskers diagrams of Dfbetas in Scenario 2, Variant i, for N = 500 and N = 5; 000. The duplicate
records are randomly drawn from the overall distribution. Box and whiskers show the distribution of Dfbetas (across 2,500
replications) for each of the coe cients in the model and for the solutions a to e.</p>
        <p>Notes: a: “Naive” estimation; b: Drop all duplicates; c: Flag and control; d: Drop superfluous duplicates; e: Weighted
regression. _cons: regression constant; x: x; z: z; t: t.</p>
        <p>Figure 4 shows that the dispersion of the estimates with
respect to the true values increases when the number of
duplicates increases. Neglecting the presence of duplicates creates
some problems when the share of duplicates reaches about
5% of the sample.</p>
        <p>For N = 500 the probabilities of obtaining biased
estimates amount to 2% and 11.4% when the doublets constitute
5% and 10% of the sample respectively. For N = 5; 000 the
same probabilities are 3% and 13%. These values are fairly
in line with the results obtained for N = 1; 500 for Variant i
(3.5% and 14%).</p>
        <p>Consistently with the results for N = 1; 500, weighting
by the inverse of the multiplicity or dropping all superfluous
duplicates are most e ective in reducing the risk of obtaining
biased estimates. Our conclusion about the influence of
duplicated records and the e ciency of the solutions does not
depend on sample size.</p>
        <p>Typical and deviant cases defined on the basis of the
distribution of the x variable. To check the robustness of
our findings, we follow the same scheme to analyze how the
position of the duplicates on the distribution of the
independent variable x (rather than the dependent variable y) a ects
regression estimates. Results are consistent with those
presented above, and are available upon request.</p>
        <p>4</p>
      </sec>
    </sec>
    <sec id="sec-20">
      <title>Conclusions</title>
      <p>
        Reliable data are a prerequisite for well grounded
analyses. In this paper we focused on the consequences of
duplicate records for regression estimates. A review of the
literature shows that there are no papers dealing with this
topic. Yet, two recent independent studies by
        <xref ref-type="bibr" rid="ref16">Slomczynski
et al. (2017)</xref>
        and by
        <xref ref-type="bibr" rid="ref11">Kuriakose and Robbins (2016)</xref>
        raised the
awareness about the quality of survey data and they warned
about the possible consequences of ignoring the presence of
duplicate records. The two teams of researchers showed that
a number of widely used surveys is a ected by duplicate
records to varying degrees. Unfortunately, little is known
about the bias and e ciency loss induced by duplicates in
survey data. Present paper partly fills this gap by
addressing two research questions: first, how do duplicates a ect
regression estimates? Second, how e ective are the possible
solutions to deal with duplicates?
      </p>
      <p>To this aim we created an artificial data set of N = 1; 500
observations and four variables with a known covariance
matrix. We adopted a Monte Carlo simulation with 2; 500
replications to investigate the consequences of 36 patterns (3
scenarios 3 cases in each scenario 4 variants) of duplicate
records. The scenarios included: (1) multiple duplications of
a single record: sextuplet, octuplet and decuplet; (2)
multiple doublets (16, 40, 79, corresponding to 2%, 5%, and 10%
of the sample); and (3) multiple quintuplets (7, 16, 31,
corresponding to 2%, 5%, and 10% of the sample). The four
variants allowed us to investigate whether the reliability of
regression estimates changed when the duplicates were
situated in specific parts of the data distribution: (i) on the whole
distribution, (ii) around the median, (iii) on the lower tie, and
(iv) on the upper tie of the distribution of the dependent
variable.</p>
      <p>For each of the scenarios we run a “naive” estimation,
which ignored the presence of duplicate records. This
allowed us to investigate the consequences of duplicate records
for regression estimates. Specifically, we investigated the
percentage bias, the standard errors, the risk of obtaining
biased estimates, and the root mean square error (RMSE) to
understand under which conditions, and to which extent the
presence of duplicates is problematic.</p>
      <p>The results showed that duplicates may bias regression
estimates when duplicate records are located in specific parts
of the distribution. In other words, the bias was null when
the duplicates were randomly drawn from the overall
distribution of the dependent variable (Variant i). Interestingly,
duplicating “typical” cases (Variant ii) was just as
problematic as duplicating “deviant” cases (Variants iii and iv). In
our simulation the bias was rather low: it reached the highest
value of about 7% when the data contained 31 quintuplets.
Overall, the bias increased with the share of duplicates in the
data, and it was higher for quintuplets than for doublets.</p>
      <p>The presence of duplicates in the data a ected also the
standard errors, and therefore the confidence intervals.
Similarly as in the case of the percentage bias, duplicates
randomly chosen from the overall distribution (Variant i) did not
a ect standard errors. Duplicating “typical” cases (Variant
ii) biased the standard errors downwards, thus increasing the
statistical power of the estimates. On the contrary, the
presence of “deviant” cases (Variants iii and iv) biased the
standard errors upwards, thus producing less precise estimates.
The bias of standard errors was overall low (up to maximum
3%), and it was higher when more duplicates were present in
the data.</p>
      <p>The presence of duplicates also a ected the risk of
obtaining biased estimates. We considered as biased the
coe cients that departed by at least 0.5 standard errors from
the true value. The risk of obtaining biased estimates
increased with the share of duplicates in the data, reaching the
values between 44%–59% (depending on the Variant) when
31 quintuplets were present in the data. The risk was also
higher when the duplicates were located on the ties of the
distribution (Variants iii and iv), and it was the lowest when
the duplicates were located in the center of the distribution
(Variant ii). Also the pattern of duplicates mattered, with
quintuplets being more problematic than doublets.</p>
      <p>The above results are interesting in the light of previous
studies which discussed the possible consequences of
duplicates for regression estimates. We found no evidence of
attenuation bias, which suggests that duplicates do not
introduce random noise in the data. Moreover, we did not find any
bias when duplicates were located randomly on the overall
distribution. On the other hand, if the duplicates were
located in a specific part of the distribution, the bias was
systematic. Moreover, we found that duplicates increased the
statistical power of estimates if the duplicated cases were
located in the center of the distribution. On the contrary, when
duplicates were located on the ties of the distribution, they
biased the confidence intervals upwards. We also found that
duplication of “typical” cases is as problematic as
duplication of “deviant” cases. It may be even considered more
problematic because the bias produced by “typical”
duplicates is accompanied by narrower confidence intervals, i.e.
higher statistical significance. On the other hand, biased
coe cients produced by “deviant” duplicates are accompanied
by broader confidence intervals.</p>
      <p>
        The number and patterns of duplicate records used in this
analysis are consistent with those identified by
        <xref ref-type="bibr" rid="ref16">Slomczynski
et al. (2017)</xref>
        , and they can, therefore, be regarded as realistic.
Hence, our first conclusion is that although the bias and e
ciency loss related to duplicate records are small, duplicates
create a risk of obtaining biased estimates. Thus, researchers
who use data with duplicate records risk to reach misleading
conclusions.
      </p>
      <p>The second goal of our analysis was to investigate the
efficacy of four solutions to reduce the e ect of duplicates on
estimation results. They included: (b) dropping all duplicates
from the sample; (c) flagging duplicates and controlling for
them in the estimation; (d) dropping all superfluous
duplicates; (e) weighting the observations by the inverse of the
duplicates’ multiplicity.</p>
      <p>The techniques that performed the best are solutions d and
e, which basically reduced the bias of the coe cient to zero.
They also performed well in reducing the risk of obtaining
biased estimates. The downside is that these solutions
biased upwards the estimated standard errors. Hence, although
dropping the superfluous duplicates or weighting the
observations by the inverse of the duplicates’ multiplicity allow to
obtain unbiased coe cients, these solutions come at the cost
of decreasing the statistical power of estimates.</p>
      <p>The solution which performed the worst was flagging
duplicates and controlling for them in the estimation. It
produced coe cients’ estimates that were more biased than
those obtained in the naive estimation. Additionally, it
systematically underestimated the standard errors. This is a
particularly worrisome combination because biased coe cients
were associated to a higher statistical confidence.</p>
      <p>Hence, the second conclusion from our study is that
weighting the duplicates by the inverse of their
multiplicity or dropping the superfluous duplicates are the best
solutions among the considered ones. These solutions
outperform all the others in reducing the percentage bias, in
reducing the risk of obtaining biased estimates, and minimizing the
RMSE. Unfortunately, they are associated to larger standard
errors, and therefore to lower statistical power. Flagging
duplicates and controlling for them is consistently a worst
solution, and in some cases (especially for Scenario 2) it produces
a higher bias, narrower confidence intervals, higher risk of
obtaining biased estimates, and greater e ciency loss than
the “naive” estimation.</p>
      <p>Our results do not depend on the sample size we chose
(N = 1; 500): they do not change whether we use a smaller
(N = 500) or a larger (N = 5; 000) sample. Similarly, the
results do not change if the variants are defined on the basis
of one of the independent variables in the regression rather
than the dependent one.</p>
      <p>These are the first results documenting the e ect of
duplicates for survey research, and they pave the road for further
research on the topic. For instance, our study considered an
ideal case in which the model used by researchers perfectly
fitted the relationship in the data, i.e. all relevant
predictors were included in the model. This is an unusual
situation in social research. Second, our study did not account
for heterogeneity of populations. We analyze a case when
the relationships of interest are the same for all respondents.
In other words, we considered a situation without
unmodeled interactions among variables. Third, in our model the
records which were substituted by duplicates (the interviews
which would have been conducted if no duplicates were
introduced in the data) were selected randomly. In reality this
is probably not the case, as these are likely the respondents
who are the most di cult to reach by interviewers.
Plausibly, the omitted variables, the heterogeneity of the
population, and the non-random choice of the interviews replaced
by the duplicates exacerbate the impact of duplicates on
regression coe cients. This suggests that our estimates of the
e ect of duplicates on percentage bias, standard errors, risk
of obtaining biased estimates, and e ciency loss are in many
aspects conservative. Moreover, our study assumed that
nonunique records were duplicates of true interviews and not
purposefully generated fakes. Addressing these limitations
is a promising path for future research.</p>
      <p>Overall, our results emphasize the importance of
collecting data of high quality, because correcting the data with
statistical tools is not a trivial task. This calls for further
research about how to address the presence of duplicates in
the data and for more refined statistical tools to minimize the
consequent bias of coe cients and standard errors, the risk
of obtaining biased estimates, and the e ciency loss.</p>
    </sec>
    <sec id="sec-21">
      <title>Acknowledgements</title>
      <p>The authors wish to thank Kazimierz M. Slomczynski,
Przemek Powałko, and the participants to the Harmonization
Project of the Polish Academy of Science for their comments
and suggestions. Possible errors or omissions are entirely the
responsibility of the authors who contributed equally to this
work.</p>
      <sec id="sec-21-1">
        <title>Appendix A Descriptive statistics for the simulated data sets. (see table A1 below)</title>
      </sec>
      <sec id="sec-21-2">
        <title>Appendix B Percentage bias for the remaining coe cients (see tables B1–B3 below)</title>
      </sec>
      <sec id="sec-21-3">
        <title>Appendix C Standard errors for the remaining coe cients (see tables C1–C3 below)</title>
      </sec>
      <sec id="sec-21-4">
        <title>Appendix D Root mean square error for the remaining coe cients (see tables D1–D3 below)</title>
        <p>Table A1
Descriptive statistics for the initial data set and for exemplary simulated data sets.</p>
      </sec>
      <sec id="sec-21-5">
        <title>N. of duplicates</title>
        <p>Variables
sd
min
max
missing</p>
        <sec id="sec-21-5-1">
          <title>Initial data set</title>
          <p>0
0
0
0
Scenario 1
1 sextuplet
1 sextuplet
1 sextuplet
1 sextuplet
1 sextuplet
1 octuplet
1 octuplet
1 octuplet
1 octuplet
1 octuplet</p>
        </sec>
      </sec>
      <sec id="sec-21-6">
        <title>1 decuplet</title>
        <p>1 decuplet
1 decuplet
1 decuplet
1 decuplet
Scenario 2
16 doublets
16 doublets
16 doublets
16 doublets
16 doublets
40 doublets
40 doublets
40 doublets
40 doublets
40 doublets
79 doublets
79 doublets
79 doublets
79 doublets
79 doublets
Scenario 3
7 quintuplets
7 quintuplets
7 quintuplets
7 quintuplets
7 quintuplets
16 quintuplets
16 quintuplets
16 quintuplets
16 quintuplets
16 quintuplets
31 quintuplets
31 quintuplets
31 quintuplets
31 quintuplets
31 quintuplets
y
x
z
t
y
x
z
t
duplicates (flag)
y
x
z
t
duplicates (flag)
y
x
z
t
duplicates (flag)
y
x
z
t
duplicates (flag)
y
x
z
t
duplicates (flag)
y
x
z
t
duplicates (flag)
y
x
z
t
duplicates (flag)
y
x
z
t
duplicates (flag)
y
x
z
t
duplicates (flag)
mean
5:213
48:04
4:916
40:03
2:588
16:86
2:402
13:35
5:212 2:587
47:99 16:83
4:911 2:400
40:01 13:33
0:00400 0:0631
5:225 2:588
47:96 16:89
4:930 2:403
39:90 13:43
0:00533 0:0729
5:187 2:595
47:93 16:87
4:909 2:393
39:98 13:28
0:00667 0:0814
5:217 2:582
48:06 16:92
4:933 2:409
40:00 13:32
0:0213 0:145
5:219 2:599
48:18 16:81
4:929 2:410
39:94 13:44
0:0533 0:225
5:227
47:99
4:896
40:01
0:105
2:582
16:94
2:404
13:42
0:307
5:219 2:584
48:09 16:87
4:932 2:404
39:81 13:47
0:0233 0:151
5:240 2:631
48:08 16:71
4:918 2:453
39:91 13:44
0:0533 0:225
5:198
48:15
4:941
39:71
0:103
2:598
16:61
2:458
13:39
0:304
3:878 14:02
14:13 99:64
3:839 13:99
5:743 90:41
3:878 14:02
14:13 99:64
3:839 13:99
5:743 90:41
0 1
3:878 14:02
14:13 99:64
3:839 13:99
5:743 90:41
0 1
3:878 14:02
14:13 99:64
3:839 13:99
5:743 90:41
0 1
3:878 14:02
14:13 99:64
3:839 13:99
5:743 90:41
0 1
3:878 14:02
14:13 99:64
3:839 13:99
5:743 90:41
0 1
3:878 14:02
14:13 99:64
3:839 13:99
5:743 90:41
0 1
3:878 14:02
14:13 99:64
3:839 13:99
5:743 90:41
0 1
3:878 14:02
14:13 99:64
3:839 13:99
5:743 90:41
0 1
3:878 14:02
14:13 97:58
3:839 13:99
5:743 90:41
0 1
obs
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Table B1</p>
        <sec id="sec-21-6-1">
          <title>Percentage bias of the</title>
          <p>z coe
cient (expressed as a percentage of z)
Solution
(a) “Naive”
estimation
0:3
0:3
0:5
0:3
0:2
0:5
0:9
2:3
4:7
0:7
1:9
3:5
1:1
2:7
5:3
0:2
0:1
0:2
1:5
3:6
6:8
1:2
2:6
4:8
2:0
4:5
7:0
0:0
0:1
0:0
0:0
0:1
0:0
0:9
2:3
4:6
0:7
1:9
4:1
1:1
2:8
6:0
0:4
0:8
1:9
0:3
0:8
1:5
0:5
1:2
2:1
0:0
0:1
0:0
0:0
0:1
0:0
0:8
2:2
4:6
1:5
4:0
8:2
2:1
5:0
10:3
0:1
0:1
0:2
1:2
3:3
6:6
1:1
2:9
5:5
1:3
3:2
6:6
0:0
0:2
0:2
0:0
0:2
0:5
0:0
0:1
0:0
0:0
0:2
0:2
0:0
0:2
0:5
0:0
0:1
0:0
Table B2</p>
        </sec>
        <sec id="sec-21-6-2">
          <title>Percentage bias of the t coe cient (expressed as a percentage of t)</title>
          <p>Solution
(a) “Naive”
estimation
0:2
0:3
0:6
0:2
0:3
0:8
0:4
0:4
0:4
0:1
0:2
0:2
0:9
2:4
4:8
0:9
1:9
3:7
0:9
2:6
5:0
0:0
0:1
0:4
1:6
3:8
7:2
1:4
2:9
5:0
1:7
4:3
7:3
0:1
0:1
0:1
1:0
2:2
4:4
0:7
2:2
4:6
1:0
3:1
6:0
0:3
1:0
1:6
0:3
0:8
1:6
0:4
1:1
2:3
0:1
0:1
0:1
0:8
2:3
4:7
1:9
5:2
10:7
2:1
5:8
11:0
1:3
3:6
7:0
1:5
3:6
7:6
1:6
3:7
7:6
0:0
0:2
0:4
0:1
0:0
0:3
0:0
0:0
0:3
0:1
0:0
0:3
0:0
0:2
0:4
0:1
0:0
0:3
0:0
0:0
0:3
0:1
0:0
0:3
Table B3
Percentage bias of the intercept (expressed as a percentage of the intercept)
Solution
(a) “Naive”
estimation
(b) Drop
all</p>
          <p>(c) Flag
and control</p>
          <p>(d) Drop
superfluous
(e) Weighted
regression</p>
        </sec>
        <sec id="sec-21-6-3">
          <title>Scenario 1</title>
          <p>Variant i: “unconstrained”
1 sextuplet
1 octuplet
1 decuplet
Variant ii: “typical”
1 sextuplet
1 octuplet
1 decuplet
Variant iii: “deviant”
1 sextuplet
1 octuplet
1 decuplet
Variant iv: “deviant”
1 sextuplet
1 octuplet
1 decuplet</p>
        </sec>
        <sec id="sec-21-6-4">
          <title>Scenario 2</title>
          <p>Variant i: “unconstrained”
16 doublets
40 doublets
79 doublets
Variant ii: “typical”
16 doublets
40 doublets
79 doublets
Variant iii: “deviant”
16 doublets
40 doublets
79 doublets
Variant iv: “deviant”
16 doublets
40 doublets
79 doublets
0:1
0:2
0:3
0:2
0:2
0:3
0:5
1:2
2:5
0:5
1:3
2:5
0:1
0:1
0:2
0:9
1:9
3:8
0:9
1:9
4:0
0:5
1:2
2:4
0:5
1:3
2:7
0:2
0:5
0:9
0:2
0:5
1:0
0:1
0:3
0:7
0:8
2:0
4:1
0:2
0:4
0:8
0:5
1:2
2:2
Table C1
Average standard error of z coe cient (expressed as a percentage of the true
standard error of z).</p>
          <p>Solution
(a) “Naive”
estimation</p>
        </sec>
        <sec id="sec-21-6-5">
          <title>Scenario 1</title>
          <p>Variant i: “unconstrained”
1 sextuplet 100:0
1 octuplet 100:0
1 decuplet 100:0
Variant ii: “typical”
1 sextuplet 99:9
1 octuplet 99:8
1 decuplet 99:8
Variant iii: “deviant”
1 sextuplet 100:1
1 octuplet 100:2
1 decuplet 100:2
Variant iv: “deviant”
1 sextuplet 100:1
1 octuplet 100:2
1 decuplet 100:3</p>
        </sec>
        <sec id="sec-21-6-6">
          <title>Scenario 2</title>
          <p>Variant i: “unconstrained”
16 doublets 100:0
40 doublets 100:0
79 doublets 100:0
Variant ii: “typical”
16 doublets 99:6
40 doublets 98:9
79 doublets 97:7
Variant iii: “deviant”
16 doublets 100:4
40 doublets 101:0
79 doublets 101:8
Variant iv: “deviant”
16 doublets 100:4
40 doublets 101:1
79 doublets 102:1</p>
        </sec>
        <sec id="sec-21-6-7">
          <title>Scenario 3</title>
          <p>Variant i: “unconstrained”
7 quintuplets 100:0
16 quintuplets 100:0
31 quintuplets 100:0
Variant ii: “typical”
7 quintuplets 99:3
16 quintuplets 98:3
31 quintuplets 96:6
Variant iii: “deviant”
7 quintuplets 100:7
16 quintuplets 101:5
31 quintuplets 102:7
Variant iv: “deviant”
7 quintuplets 100:8
16 quintuplets 101:7
31 quintuplets 103:0
101:1
102:8
105:7
101:5
103:9
108:1
100:7
101:7
103:3
100:6
101:5
103:0
99:7
99:0
97:8
99:0
97:3
94:4
99:0
97:3
94:4
99:4
98:4
96:8
99:2
98:1
96:1
99:3
98:1
96:2
100:5
101:4
102:7
100:6
101:4
102:9
100:9
102:2
104:3
101:9
102:0
102:0
101:9
102:0
102:1
101:9
101:9
102:0
101:9
101:9
102:0
102:0
102:4
103:1
102:2
102:9
104:0
101:9
102:0
102:3
101:8
101:9
102:0
102:5
103:5
105:3
102:7
103:8
105:9
102:4
103:3
104:7
102:4
103:2
104:6
Table C2
Average standard error of t coe cient (expressed as a percentage of the true
standard error of t).</p>
          <p>Solution
(a) “Naive”
estimation</p>
          <p>(c) Flag
and control</p>
          <p>(d) Drop
superfluous
(e) Weighted
regression</p>
        </sec>
        <sec id="sec-21-6-8">
          <title>Scenario 2</title>
          <p>Variant i: “unconstrained”
16 doublets 100:0
40 doublets 100:0
79 doublets 100:0
Variant ii: “typical”
16 doublets 99:6
40 doublets 98:9
79 doublets 97:8
Variant iii: “deviant”
16 doublets 100:4
40 doublets 101:0
79 doublets 101:9
Variant iv: “deviant”
16 doublets 100:4
40 doublets 101:0
79 doublets 101:9</p>
        </sec>
        <sec id="sec-21-6-9">
          <title>Scenario 3</title>
          <p>Variant i: “unconstrained”
7 quintuplets 100:0
16 quintuplets 100:0
31 quintuplets 100:0
Variant ii: “typical”
7 quintuplets 99:3
16 quintuplets 98:3
31 quintuplets 96:7
Variant iii: “deviant”
7 quintuplets 100:7
16 quintuplets 101:5
31 quintuplets 102:7
Variant iv: “deviant”
7 quintuplets 100:7
16 quintuplets 101:5
31 quintuplets 102:6
100:6
101:6
103:2
100:7
101:6
103:2
100:0
100:0
100:1
99:7
99:0
97:9
99:0
97:3
94:5
98:9
97:2
94:1
100:0
100:0
100:1
99:4
98:4
96:9
99:3
98:1
96:1
99:2
97:9
95:7
101:0
102:2
104:4
106:6
106:6
106:7
106:6
106:7
106:7
106:5
106:6
106:7
106:5
106:6
106:7
106:7
107:1
107:8
106:9
107:6
108:8
106:5
106:5
106:8
106:5
106:6
106:8
107:2
108:2
110:1
107:4
108:6
110:7
107:1
107:9
109:4
107:1
108:0
109:5
Table C3
Average standard error of the intercept (expressed as a percentage of the true
standard error of the intercept).</p>
          <p>Solution
(a) “Naive”
estimation
(b) Drop
all</p>
          <p>(c) Flag
and control</p>
          <p>(d) Drop
superfluous
(e) Weighted
regression
Variant i: “unconstrained”
1 sextuplet 100:0
1 octuplet 100:0
1 decuplet 100:0
Variant ii: “typical”
1 sextuplet 99:9
1 octuplet 99:8
1 decuplet 99:8
Variant iii: “deviant”
1 sextuplet 100:1
1 octuplet 100:2
1 decuplet 100:3
Variant iv: “deviant”
1 sextuplet 100:1
1 octuplet 100:2
1 decuplet 100:2</p>
        </sec>
        <sec id="sec-21-6-10">
          <title>Scenario 2</title>
          <p>Variant i: “unconstrained”
16 doublets 100:0
40 doublets 100:0
79 doublets 100:0
Variant ii: “typical”
16 doublets 99:6
40 doublets 98:9
79 doublets 97:8
Variant iii: “deviant”
16 doublets 100:5
40 doublets 101:1
79 doublets 102:1
Variant iv: “deviant”
16 doublets 100:4
40 doublets 101:0
79 doublets 101:8</p>
        </sec>
        <sec id="sec-21-6-11">
          <title>Scenario 3</title>
          <p>Variant i: “unconstrained”
7 quintuplets 100:0
16 quintuplets 100:0
31 quintuplets 100:0
Variant ii: “typical”
7 quintuplets 99:3
16 quintuplets 98:3
31 quintuplets 96:7
Variant iii: “deviant”
7 quintuplets 100:8
16 quintuplets 101:7
31 quintuplets 103:0
Variant iv: “deviant”
7 quintuplets 100:7
16 quintuplets 101:5
31 quintuplets 102:5
100:6
101:5
103:0
100:7
101:7
103:3
101:0
102:3
104:6
101:0
102:4
104:8
100:1
100:2
100:3
99:7
99:2
98:2
99:0
97:4
94:6
99:0
97:2
94:3
100:0
100:1
100:3
99:5
98:6
97:1
99:3
98:2
96:3
99:2
97:9
95:8
103:9
104:0
104:1
104:0
104:0
104:1
103:9
104:0
104:0
103:9
104:0
104:0
104:1
104:5
105:2
104:3
105:0
106:1
103:9
104:0
104:3
103:9
104:0
104:1
104:6
105:6
107:4
104:7
105:9
108:0
104:4
105:3
106:8
104:4
105:2
106:7
Table D1</p>
        </sec>
        <sec id="sec-21-6-12">
          <title>Normalized RMSE for the</title>
          <p>z coe
cient
Solution
(a) “Naive”
estimation
(b) Drop
all
(c) Flag
and control
(d) Drop
superfluous
(e) Weighted
regression
14:8
14:8
14:8
14:7
14:7
14:7
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:7
14:8
15:2
14:8
15:0
15:4
14:9
15:2
16:0
14:8
14:8
14:8
14:7
14:9
15:8
14:9
15:2
15:9
15:0
15:7
16:7
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:9
15:2
15:6
15:0
15:5
16:6
14:9
15:1
15:8
14:9
15:2
16:3
14:9
15:2
15:6
15:0
15:3
15:8
14:9
15:1
15:5
14:9
15:2
15:6
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:7
14:8
15:2
14:7
14:9
16:2
14:8
15:2
17:3
14:8
14:8
14:8
14:7
14:9
15:7
14:7
14:8
15:2
14:7
14:8
15:7
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
14:8
15:0
15:2
14:8
15:0
15:2
14:8
15:0
15:2
14:8
15:0
15:2
14:9
15:1
15:4
14:9
15:1
15:4
14:9
15:1
15:4
14:9
15:1
15:4
15:0
15:1
15:1
15:0
15:1
15:1
15:0
15:1
15:1
15:0
15:1
15:1
15:1
15:1
15:2
15:1
15:2
15:4
15:0
15:1
15:1
15:0
15:0
15:1
15:1
15:3
15:5
15:2
15:3
15:6
15:1
15:2
15:5
15:1
15:2
15:4
Table D2</p>
        </sec>
        <sec id="sec-21-6-13">
          <title>Normalized RMSE for the t coe</title>
          <p>cient
Solution
(a) “Naive”
estimation
(b) Drop
all</p>
          <p>(c) Flag
and control
(d) Drop
superfluous
(e) Weighted
regression
Variant i: “unconstrained”
1 sextuplet
1 octuplet
1 decuplet
Variant ii: “typical”
1 sextuplet
1 octuplet
1 decuplet
Variant iii: “deviant”
1 sextuplet
1 octuplet
1 decuplet
Variant iv: “deviant”
1 sextuplet
1 octuplet
1 decuplet
24:3
24:3
24:3
24:3
24:3
24:2
24:3
24:3
24:4
24:3
24:3
24:4
24:3
24:3
24:3
24:2
24:2
24:2
24:4
24:6
25:0
24:4
24:7
25:3
24:3
24:3
24:3
24:2
24:2
24:6
24:5
24:8
25:5
24:5
25:0
26:0
24:3
24:4
24:4
24:4
24:4
24:4
24:3
24:4
24:4
24:3
24:4
24:4
24:6
25:0
25:7
24:7
25:3
26:6
24:5
24:8
25:5
24:5
24:9
25:8
24:6
25:0
25:7
24:6
25:1
25:9
24:5
24:9
25:5
24:5
24:9
25:5
24:3
24:3
24:3
24:3
24:3
24:3
24:3
24:3
24:3
24:3
24:3
24:3
24:3
24:3
24:3
24:2
24:2
24:2
24:1
24:2
25:3
24:1
24:3
25:4
24:3
24:3
24:3
24:2
24:2
24:5
24:2
24:1
24:6
24:2
24:1
24:5
24:3
24:4
24:4
24:3
24:4
24:4
24:3
24:4
24:4
24:3
24:4
24:4
24:4
24:6
25:0
24:4
24:6
24:9
24:4
24:6
25:0
24:4
24:6
25:0
24:5
24:8
25:4
24:5
24:8
25:4
24:5
24:8
25:4
24:5
24:8
25:4
25:9
25:9
25:9
25:9
25:9
25:9
25:9
25:9
25:9
25:9
25:9
25:9
25:9
26:0
26:2
26:0
26:1
26:4
25:9
25:9
25:9
25:9
25:9
26:0
26:1
26:3
26:8
26:1
26:4
26:9
26:0
26:2
26:6
26:0
26:2
26:6
Table D3</p>
        </sec>
        <sec id="sec-21-6-14">
          <title>Normalized RMSE for the intercept</title>
          <p>Solution
(a) “Naive”
estimation
(b) Drop
all
(c) Flag
and control
(d) Drop
superfluous
(e) Weighted
regression</p>
        </sec>
        <sec id="sec-21-6-15">
          <title>Scenario 1</title>
          <p>Variant i: “unconstrained”
1 sextuplet
1 octuplet
1 decuplet
Variant ii: “typical”
1 sextuplet
1 octuplet
1 decuplet
Variant iii: “deviant”
1 sextuplet
1 octuplet
1 decuplet
Variant iv: “deviant”
1 sextuplet
1 octuplet
1 decuplet</p>
        </sec>
        <sec id="sec-21-6-16">
          <title>Scenario 2</title>
          <p>Variant i: “unconstrained”
16 doublets
40 doublets
79 doublets
Variant ii: “typical”
16 doublets
40 doublets
79 doublets
Variant iii: “deviant”
16 doublets
40 doublets
79 doublets
Variant iv: “deviant”
16 doublets
40 doublets
79 doublets
5:8
5:8
5:8
5:7
5:7
5:7
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:7
5:8
5:7
5:7
5:6
5:8
5:9
6:4
5:8
5:9
6:4
5:7
5:7
5:8
5:7
5:7
5:6
5:9
6:2
7:1
5:9
6:1
7:1
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:9
6:1
5:8
6:0
6:2
5:8
6:0
6:4
5:8
6:0
6:5
5:8
5:9
6:1
5:8
5:9
6:1
5:8
5:9
6:1
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:7
5:7
5:6
5:7
5:6
5:5
5:7
5:9
6:8
5:8
5:8
5:8
5:7
5:7
5:6
5:7
5:7
5:6
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:8
5:9
5:8
5:8
5:9
5:8
5:8
5:9
5:8
5:8
5:9
5:8
5:9
6:0
5:8
5:9
6:0
5:8
5:9
6:0
5:8
5:9
6:0
6:0
6:0
6:0
6:0
6:0
6:0
6:0
6:0
6:0
6:0
6:0
6:0
6:0
6:0
6:0
6:0
6:0
6:1
6:0
6:0
6:0
6:0
6:0
6:0
6:0
6:1
6:2
6:0
6:1
6:2
6:0
6:1
6:1
6:0
6:1
6:1</p>
        </sec>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref id="ref1">
        <mixed-citation>
          <string-name>
            <surname>American Statistical Association.</surname>
          </string-name>
          (
          <year>2004</year>
          ).
          <article-title>Interviewer falsification in survey research: Current best methods for prevention, detection, and repair of its e ects</article-title>
          .
          <source>Survey Research</source>
          .
          <article-title>Newsletter from the Survey Research Laboratory, College of Urban Planning and Public A airs</article-title>
          , University of Illinois at Chicago.
          <volume>35</volume>
          (
          <issue>1</issue>
          ),
          <fpage>1</fpage>
          -
          <lpage>5</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref2">
        <mixed-citation>
          <string-name>
            <surname>Diekmann</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          (
          <year>2005</year>
          ).
          <article-title>Betrug und Täuschung in der Wissenschaft</article-title>
          . Datenfälschung, Diagnoseverfahren, Konsequenzen.
          <source>Schweizerische Zeitschrift für Soziologie</source>
          ,
          <volume>31</volume>
          (
          <issue>1</issue>
          ),
          <fpage>7</fpage>
          -
          <lpage>29</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref3">
        <mixed-citation>
          <string-name>
            <surname>Elmagarmid</surname>
            ,
            <given-names>A. K.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Ipeirotis</surname>
            ,
            <given-names>P. G.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Verykios</surname>
            ,
            <given-names>V. S.</given-names>
          </string-name>
          (
          <year>2007</year>
          ).
          <article-title>Duplicate record detection: a survey</article-title>
          .
          <source>IEEE Transactions on Knowledge and Data Engineering</source>
          ,
          <volume>19</volume>
          (
          <issue>1</issue>
          ),
          <fpage>1</fpage>
          -
          <lpage>16</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref4">
        <mixed-citation>
          <string-name>
            <surname>European Social Survey.</surname>
          </string-name>
          (
          <year>2015</year>
          ).
          <article-title>European Social Survey round 6</article-title>
          .
          <source>Bergen: Norwegian Social Science Data Services.</source>
        </mixed-citation>
      </ref>
      <ref id="ref5">
        <mixed-citation>
          <string-name>
            <surname>Ferrarini</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          (
          <year>2011</year>
          ).
          <article-title>A fitter use of Monte Carlo simulations in regression models</article-title>
          .
          <source>Computational Ecology and Software</source>
          ,
          <volume>1</volume>
          (
          <issue>4</issue>
          ),
          <fpage>240</fpage>
          -
          <lpage>243</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref6">
        <mixed-citation>
          <string-name>
            <surname>Finn</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Ranchhod</surname>
            ,
            <given-names>V.</given-names>
          </string-name>
          (
          <year>2013</year>
          ).
          <article-title>Genuine fakes: The prevalence and implications of fieldworker fraud in a large South African survey</article-title>
          .
          <source>Working Paper 115 of the Southern Africa Labour and Development Research Unit</source>
          , University of Cape Town,
          <volume>115</volume>
          .
        </mixed-citation>
      </ref>
      <ref id="ref7">
        <mixed-citation>
          <string-name>
            <surname>Fishman</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          (
          <year>2005</year>
          ).
          <article-title>A first course in Monte Carlo</article-title>
          . Duxbury Press.
        </mixed-citation>
      </ref>
      <ref id="ref8">
        <mixed-citation>
          <string-name>
            <surname>Hassanzadeh</surname>
            ,
            <given-names>O.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Miller</surname>
            ,
            <given-names>R. J.</given-names>
          </string-name>
          (
          <year>2009</year>
          ).
          <article-title>Creating probabilistic databases from duplicated data</article-title>
          .
          <source>The VLDB Journal-The International Journal on Very Large Data Bases</source>
          ,
          <volume>18</volume>
          (
          <issue>5</issue>
          ),
          <fpage>1141</fpage>
          -
          <lpage>1166</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref9">
        <mixed-citation>
          <string-name>
            <surname>Hill</surname>
            ,
            <given-names>T. P.</given-names>
          </string-name>
          (
          <year>1999</year>
          ).
          <article-title>The di culty of faking data</article-title>
          .
          <source>Chance</source>
          ,
          <volume>12</volume>
          (
          <issue>3</issue>
          ),
          <fpage>27</fpage>
          -
          <lpage>31</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref10">
        <mixed-citation>
          <string-name>
            <surname>Koczela</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Furlong</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>McCarthy</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Mushtaq</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          (
          <year>2015</year>
          ).
          <article-title>Curbstoning and beyond: confronting data fabrication in survey research</article-title>
          .
          <source>Statistical Journal of the IAOS</source>
          ,
          <volume>31</volume>
          (
          <issue>3</issue>
          ),
          <fpage>413</fpage>
          -
          <lpage>422</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref11">
        <mixed-citation>
          <string-name>
            <surname>Kuriakose</surname>
            ,
            <given-names>N.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Robbins</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          (
          <year>2016</year>
          ).
          <article-title>Don't get duped: Fraud through duplication in public opinion surveys</article-title>
          .
          <source>Statistical Journal of the IAOS</source>
          ,
          <volume>32</volume>
          (
          <issue>3</issue>
          ),
          <fpage>283</fpage>
          -
          <lpage>291</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref12">
        <mixed-citation>
          <string-name>
            <surname>Lessler</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Kalsbeek</surname>
            ,
            <given-names>W.</given-names>
          </string-name>
          (
          <year>1992</year>
          ).
          <article-title>Nonsampling error in surveys</article-title>
          . New York: Wiley.
        </mixed-citation>
      </ref>
      <ref id="ref13">
        <mixed-citation>
          <string-name>
            <surname>Schnell</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          (
          <year>1991</year>
          ).
          <article-title>Der Einfluß gefälschter Interviews auf Survey-Ergebnisse</article-title>
          .
          <source>Zeitschrift für Soziologie</source>
          ,
          <volume>20</volume>
          (
          <issue>1</issue>
          ),
          <fpage>25</fpage>
          -
          <lpage>35</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref14">
        <mixed-citation>
          <string-name>
            <surname>Schräpler</surname>
          </string-name>
          , J.
          <string-name>
            <surname>-</surname>
          </string-name>
          .
          <string-name>
            <surname>-</surname>
          </string-name>
          .-P. &amp;
          <string-name>
            <surname>Wagner</surname>
            ,
            <given-names>G. G.</given-names>
          </string-name>
          (
          <year>2005</year>
          ).
          <article-title>Characteristics and impact of faked interviews in surveys-An analysis of genuine fakes in the raw data of SOEP</article-title>
          .
          <source>Allgemeines Statistisches Archiv</source>
          ,
          <volume>89</volume>
          (
          <issue>1</issue>
          ),
          <fpage>7</fpage>
          -
          <lpage>20</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref15">
        <mixed-citation>
          <string-name>
            <surname>Schreiner</surname>
            ,
            <given-names>I.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Pennie</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Newbrough</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          (
          <year>1988</year>
          ).
          <article-title>Interviewer falsification in Census Bureau surveys</article-title>
          .
          <source>In Proceedings of the American Statistical Association (Survey Research Methods Section)</source>
          (pp.
          <fpage>491</fpage>
          -
          <lpage>496</lpage>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref16">
        <mixed-citation>
          <string-name>
            <surname>Slomczynski</surname>
            ,
            <given-names>K. M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Powałko</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          , &amp;
          <string-name>
            <surname>Krauze</surname>
            ,
            <given-names>T.</given-names>
          </string-name>
          (
          <year>2017</year>
          ).
          <article-title>Nonunique records in International Survey Projects: The need for extending data quality control</article-title>
          .
          <source>Survey Research Methods</source>
          ,
          <volume>11</volume>
          (
          <issue>1</issue>
          ),
          <fpage>1</fpage>
          -
          <lpage>16</lpage>
          . doi:doi:
          <pub-id pub-id-type="doi">10.18148/srm</pub-id>
          /
          <year>2017</year>
          .v11i1.
          <fpage>6557</fpage>
        </mixed-citation>
      </ref>
      <ref id="ref17">
        <mixed-citation>
          <string-name>
            <surname>Waller</surname>
            ,
            <given-names>L. G.</given-names>
          </string-name>
          (
          <year>2013</year>
          ).
          <article-title>Interviewing the surveyors: Factors which contribute to questionnaire falsification (curbstoning) among Jamaican field surveyors</article-title>
          .
          <source>International Journal of Social Research Methodology</source>
          ,
          <volume>16</volume>
          (
          <issue>2</issue>
          ),
          <fpage>155</fpage>
          -
          <lpage>164</lpage>
          .
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>