<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta />
    <article-meta>
      <title-group>
        <article-title>Asking Survey Respondents about Reasons for Their Behavior: A Split Ballot Experiment in Ethiopia</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>Charles Q. Lau</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Survey Research Division</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>RTI International Gretchen McHenry</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Survey Research Division</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>RTI International</string-name>
        </contrib>
      </contrib-group>
      <pub-date>
        <year>2014</year>
      </pub-date>
      <fpage>2</fpage>
      <lpage>15</lpage>
      <abstract>
        <p>When policymakers design programs and policies, they often want to understand why individuals engage in particular behaviors. Collecting survey data about respondentsʼ reasons for their behavior presents important challenges, and there is little methodological research on this topic. We conducted an experiment to investigate the best practices for asking questions about respondentsʼ reasons for their behavior. We embedded a split ballot experiment in a face-to-face survey of 608 entrepreneurs in Ethiopia. Respondents were asked questions about why they did not engage in three business practices (advertising, sharing product storage, and switching suppliers). When asked these questions, respondents were randomly assigned to one of three conditions: close-ended questions, open-ended questions with interviewer probing, and open-ended questions without probing. Respondents endorsed more responses when asked close-ended (versus open-ended) questions. Close-ended responses produced higher rates of socially undesirable responses and fewer “other” responses. Notably, probing had no effect on the number or types of responses given. Our results suggest some best practices for asking respondents questions about reasons for their behavior. The authors gratefully acknowledge support from RTI International for funding the Kal Addis Business Survey (KABS). We thank Jason Wares for assistance in designing KABS, and Efera Busa and Benyam Lemma for assistance in collecting the KABS data. We also thank Hyunjoo Park of RTI International and two anonymous reviewers for valuable comments on the paper. Any errors in this manuscript are our own.</p>
      </abstract>
      <kwd-group>
        <kwd>close-ended</kwd>
        <kwd>developing country</kwd>
        <kwd>entrepreneurs</kwd>
        <kwd>Motivation</kwd>
        <kwd>open-ended</kwd>
        <kwd>probing</kwd>
        <kwd>reason</kwd>
        <kwd>sensitive</kwd>
        <kwd>social desirability</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <fig>
      <graphic xlink:href="./data/testing/7/37078.images/img_1_1.png" />
    </fig>
    <sec id="sec-1">
      <title>Acknowledgement</title>
    </sec>
    <sec id="sec-2">
      <title>Copyright</title>
      <sec id="sec-2-1">
        <title>Introduction</title>
        <p>
          When policymakers design programs and policies, they often want to understand why
individuals act in particular ways. Although some researchers caution against asking
respondents to cite reasons why they do (or do not) engage in behaviors
          <xref ref-type="bibr" rid="ref10 ref18">(Pasek and
Krosnick 2010: 41; Wilson 2010)</xref>
          , data about respondent motivations for their behavior are
analytically useful. By understanding the causes of peopleʼs behavior, policymakers can take
steps to reduce undesirable behaviors or encourage desirable behaviors. For example,
questions in the National Health Interview Survey ask respondents why they delayed seeking
medical care, allowing researchers to understand barriers to healthcare access
          <xref ref-type="bibr" rid="ref1">(Centers for
Disease Control and Prevention, 2012)</xref>
          . The Current Population Survey also asks individuals
why they did not vote or register to vote, shedding light on mechanisms underlying political
participation
          <xref ref-type="bibr" rid="ref17">(United States Census Bureau, 2010)</xref>
          .
        </p>
        <p>Given the value of data about individualsʼ motivations for behavior, it is notable that there is
little research on best practices for designing these questions. To address this gap, we
embedded a split ballot experiment in a face-to-face survey of 608 entrepreneurs in Ethiopia.
Conducting the survey in a developing country allowed us to study this topic in a context that
poses additional challenges to asking such questions. In our survey, we randomly assigned
one of three methods for asking respondents about reasons for their behavior. The methods
differ in whether questions are close-ended versus open-ended, and whether interviewers
probed respondents. Our analysis evaluates the three methods by comparing numbers of
endorsed responses and the number of socially desirable responses in particular.</p>
      </sec>
      <sec id="sec-2-2">
        <title>Background</title>
        <p>
          There are many ways to collect survey data about reasons for respondentʼs behavior. The
method we adopt in this paper involves pre-specifying a list of possible reasons for behavior
on the instrument, and then having interviewers record whether each response applies or not
(using yes/no responses for each item).[1] When designing this type of question, researchers
must make two key decisions
          <xref ref-type="bibr" rid="ref18">(Wilson 2010)</xref>
          . First, should interviewers ask close-ended
questions—reading each possible response and then recording a yes/no response for each?
Or should interviewers ask open-ended questions and then record yes/no responses based
on the respondentʼs open-ended answer? Second, if open-ended questions are used, should
interviewers probe respondents for clarification? In the following sections, we draw from
previous literature to develop expectations about the advantages and disadvantages of
different types of close-ended and open-ended questions.
        </p>
        <sec id="sec-2-2-1">
          <title>Close-Ended Versus Open-Ended Questions</title>
          <p>
            Asking close-ended questions (rather than open-ended ones) is a form of standardized
interviewing, in which each respondent hears the exact same question and response options,
regardless of the interview flow or tone
            <xref ref-type="bibr" rid="ref13 ref2">(Converse and Schuman 1974; Schober and Conrad
2002)</xref>
            . This approach has the advantage of encouraging respondents to consider reasons
they had not previously thought about. It also encourages respondents to think about the
issue from a variety of perspectives, which may result in a greater number of endorsed
responses and may also limit “donʼt know” responses. Further, close-ended questions may
reduce respondentsʼ concerns about reporting socially undesirable answers. Reading
response options may give tacit approval for socially undesirable answers and may help
develop a sense of trust between a respondent and the interviewer. It also means that
respondents do not have to verbally state a socially undesirable admission about
themselves, which is the case in an open-ended question format.
15.01.14 09:36
          </p>
          <p>
            Close-ended questions also have disadvantages, many of which are rectified by open-ended
formats. Reading close-ended response options during the interview can be time consuming
and feel repetitive to the respondent. Open-ended questions, in contrast, may be more
engaging for respondents because they comport more with conversational norms and allow
respondents to better communicate the reasoning behind their behavior
            <xref ref-type="bibr" rid="ref5">(Fowler 1995)</xref>
            .
Open-ended questions have also been shown to solicit meaningful, salient information from
respondents
            <xref ref-type="bibr" rid="ref6 ref7">(Geer 1988; Geer 1991)</xref>
            . In addition, close-ended questions may suffer from
primacy or recency effects, where the first (or last) response options are more likely to be
endorsed, whereas primacy and recency effects are eliminated with open-ended questions.
Finally, reading response options may implicitly convey the researcherʼs values or
preferences, potentially biasing respondents in a particular direction. Open-ended questions,
in contrast, do not have this limitation, and also provide an opportunity to collect data about
issues researchers had not previously considered.
          </p>
        </sec>
        <sec id="sec-2-2-2">
          <title>Probing</title>
          <p>If open-ended questions are used, interviewers could simply select the pre-specified reasons
that apply to the respondentsʼ open-ended answer (without probing), or probe for a more
complete or detailed response. Probing can facilitate respondent comprehension of the
question and may reduce errors in the interviewerʼs coding of responses. An exchange with
the interviewer also may encourage respondents to think more deeply about their answers.
This increased engagement with the question-answer process, as well as with the
interviewer, may yield more endorsed answers, reduce respondent satisficing, and increase
reports of socially undesirable behaviors. Schaeffer and Maynard (2008) show that directive
probes or requests for confirmation from interviewers increase a respondentʼs likelihood of
reporting embarrassing or incriminating responses.</p>
          <p>There are three potential drawbacks of probing. First, probing gives interviewers more
discretion and may lead to increased interviewer errors and variance. For example, Fowler
and Mangione (1990) find that the number of probes, directive probes used, and occasions
where an interviewer failed to probe are associated with increased error. They also suggest
that probing may introduce interviewer-level variance, which decreases the efficiency of
survey estimates. However, Schober and Conradʼs (1997) small-scale experimental study
finds no evidence that probing increases interviewer error or variance. Second, probing may
increase the number of “other” responses if the interviewer cannot code the response into
one of the pre-existing categories due to the nuanced response from the respondents. Third,
the conversational nature of the interview may increase administration time, increasing
survey costs and field data collection time.</p>
          <p>In sum, the literature suggests that there are advantages and disadvantages of using
close-ended versus open-ended questions, as well as probing versus not probing. Given the
lack of research in this area, we designed a split ballot experiment to investigate the quality
of data produced by three methods.</p>
        </sec>
      </sec>
      <sec id="sec-2-3">
        <title>Experimental Design</title>
        <sec id="sec-2-3-1">
          <title>Data</title>
          <p>We analyze data from the Kal Addis Business Survey (KABS), a paper-and-pencil interview
of 608 entrepreneurs in the Ethiopian capital of Addis Ababa. Eligible respondents were
owners or senior managers of small and medium businesses (between 3 and 99 employees)
based in Addis Ababa. Examples of businesses in the sample include a restaurant, car repair
shop, and a textile manufacturer. The purpose of KABS was to improve sampling and
questionnaire design methodologies in developing countries, particularly for surveys of
entrepreneurs. The survey measured entrepreneursʼ attitudes and business practices, and
15.01.14 09:36
included questions about purchasing raw materials from suppliers, advertising, product
storage, among other topics. Professional Ethiopian interviewers with at least three years of
interviewing experience administered the survey in the Amharic language in the summer of
2012. All interviewers also participated in a three day training and pre-test of the instrument.
Throughout data collection, survey managers held quality review meetings with interviewers
to enhance standardization and to answer questions about field implementation. The mean
administration time was 29 minutes (standard deviation = 9 minutes).</p>
          <p>
            Because a sampling frame of entrepreneurs was not available in Addis Ababa, KABS used
respondent-driven sampling (RDS). RDS is a method of chain referral sampling that
combines a snowball sample with a mathematical model that adjusts for the non-random
selection of the initial set of respondents
            <xref ref-type="bibr" rid="ref9">(Heckathorn 1997)</xref>
            . To implement RDS in KABS, we
initially recruited a convenience sample of 24 individuals through personal networks. These
individuals were interviewed and then provided with three invitations to recruit up to three
individuals to participate in the study. Each additional wave of recruits was asked to recruit
up to three additional individuals. Recruited individuals contacted the field data collection
teams, who then scheduled and conducted the interview in a location of the respondentʼs
choosing. We provided a leather wallet to respondents for completing the survey and mobile
phone airtime for referring others to the study. Because our focus is on the internal validity of
the split ballot experiment, we do not apply weights from the RDS in the paper.
Characteristics of the sample are presented in Table 1.
          </p>
          <p>Three-quarters of respondents are male with an average age of 31 years old, reflecting the
young age of the Ethiopian population. The majority of respondents are owners of the
business (82%) versus managers (18%). The sample is comprised of businesses in the
manufacturing (14%), service (48%), and trade (39%) sectors. The vast majority of
businesses were profitable in the past year, and on average, businesses had eight
employees and were six years old.</p>
        </sec>
        <sec id="sec-2-3-2">
          <title>Respondent Characteristics Gender (n = 608)</title>
          <p>Male
Female
Did not complete secondary
Secondary school
Vocational or some university
Graduate degree or higher</p>
        </sec>
        <sec id="sec-2-3-3">
          <title>Business Characteristics Sector (n = 608)</title>
          <p>Manufacturing
Service
Trade</p>
          <p>Total %
Annual revenue in dollars (n = 539)
Less than $2778
$2778 – $5,555
$5,556 – $13,889
$13,890 – $41,667
Over $41,667
Position in business (n = 608)
Total %
Owner
Senior day-to-day manager
Age in years (n = 608)
Standard deviation
Hours worked/week (n = 596)
Standard deviation
82%
18%</p>
          <p>Standard deviation</p>
        </sec>
        <sec id="sec-2-3-4">
          <title>Mean business age (years) (n = 606)</title>
          <p>Standard deviation
Note: The total sample size for KABS sample is 608. The valid sample size for each
variable is indicated in table. Percentages may not sum to 100 due to rounding.
KABS included questions about three business practices: advertising, switching to a new
supplier to buy raw materials, and sharing product storage with another business. We
present the exact question wording for these questions in the Appendix. These three
practices facilitate economic growth and are practices that policymakers would like to
encourage in developing countries. Therefore, understanding why individuals do not engage
in these practices is important for policymakers who design interventions to stimulate
economic growth. In different parts of the interview, respondents were asked if they engaged
in these business practices. Those who said they did not take part in each business practice
were asked why not. We generated pre-specified reasons for each behavior during formative
research, which involved in-depth interviews with entrepreneurs, as well as a review of
literature on entrepreneurship in Ethiopia. We modified these reasons throughout the
pre-testing process.</p>
        </sec>
        <sec id="sec-2-3-5">
          <title>Split Ballot Design</title>
          <p>We developed three separate instruments, each with a different method of asking questions
about reasons for respondentʼs behavior. Respondents were randomly assigned to one of
three methods (Table 2). Each respondent was assigned to the same method across all three
business practices based on their respondent ID number (itself randomly assigned). The
randomization was successful in that there were no significant correlations between
questionnaire version and respondent or business characteristics. Full tables are available
from the authors upon request.</p>
          <p>
            Table 2 shows that in the close-ended method, interviewers read every pre-specified
response option while asking respondents a series of yes/no questions about whether the
option applied or not. This method is the norm in social surveys and reflects standardized
interviewing practices
            <xref ref-type="bibr" rid="ref8">(Groves et al. 2009)</xref>
            . We read the potential reasons orally (rather than
using a showcard) because of the survey populationʼs lower levels of literacy and
unfamiliarity with showcards.
15.01.14 09:36
          </p>
        </sec>
        <sec id="sec-2-3-6">
          <title>Close-ended</title>
        </sec>
        <sec id="sec-2-3-7">
          <title>Open-ended with Open-ended probing without probing</title>
          <p>Interviewer reads
response options
Interviewer probes
Number of respondents</p>
          <p>Yes
If needed
203</p>
          <p>No
Yes
203</p>
          <p>
            No
No
202
In the open-ended with probing method, interviewers asked an open-ended question instead
of reading the response options. The interviewer then coded the respondentʼs open-ended
answer into the pre-specified options, and probed the respondent as needed. The interviewer
did not record the verbatim open-ended response. Interviewers were trained to adopt
conversational interviewing practices when probing
            <xref ref-type="bibr" rid="ref11">(Schober 1998)</xref>
            , and used non-directive,
neutral probes to clarify unclear or inadequate responses. Examples of probes included
repeating the question, asking a general question, or asking a respondent to clarify a
response. In the open-ended without probing method, interviewers asked an open-ended
question, coded the open-ended data into the pre-specified response options, and did not
probe. Again, the interviewer did not collect the verbatim response. This method combines
elements of standardization (i.e., no interviewer-respondent discussion) and conversational
interviewing (i.e., interviewer has discretion to select the appropriate response.) All three
methods contained an “other (specify)” response. During preliminary analysis, we recoded
some “other” responses into existing pre-specified categories or created new categories
when the other (specify) meaning was unambiguous.
          </p>
        </sec>
        <sec id="sec-2-3-8">
          <title>Hypotheses</title>
          <p>Our analysis seeks to identify the method that produces the most useful data about why
individuals do not engage in the three business practices. Because obtaining validation data
for this type of information is difficult, we focus on the number of endorsed responses and
socially undesirable responses in particular. Another possible indicator is timing data, but
because KABS used paper-and-pencil interviewing (like most surveys in developing
countries), timing data on individual questions were not available. Below, we describe each
indicator and present hypotheses.</p>
          <p>
            Number of endorsed responses: The number of responses that respondents select is
indicative of greater engagement with the subject matter. A greater number of responses is
also analytically useful because it helps analysts understand multiple influences on behavior.
Hypothesis 1: The close-ended method will result in greater number of endorsed reasons
than either open-ended method because respondents must consider each option separately.
Support for this reasoning comes from the web survey literature, which shows that
respondents endorse more responses when presented with a yes/no matrix (that requires an
answer for each response) rather than a “check all” list
            <xref ref-type="bibr" rid="ref15">(Smyth et al. 2006)</xref>
            .
15.01.14 09:36
          </p>
          <p>
            Hypothesis 2: Open-ended with probing will lead to a greater number of endorsements than
open-ended without probing. During probing, interviewers may encourage respondents to
think about the issue from multiple angles and therefore provide more responses.
Socially undesirable reporting: We assume that respondents are reluctant to endorse
responses that are socially undesirable, and that increases in socially undesirable reporting
reflect a more preferable method. This logic has been widely used in other areas, such as
mode effects on reports of sexual activity
            <xref ref-type="bibr" rid="ref16">(Tourangeau and Smith 1996)</xref>
            and smoking
            <xref ref-type="bibr" rid="ref3">(Currivan et al. 2004)</xref>
            . We include a range of socially undesirable measures in our study,
ranging from more sensitive (e.g., reporting distrust of others) to less sensitive (e.g.,
reporting lack of knowledge about an issue).
          </p>
          <p>Hypothesis 3: The close-ended method will yield more socially undesirable reporting than
either open-ended method because the interviewer-supplied responses give tacit approval to
the possibility of the response. In addition, the respondent only has to say “yes” to endorse a
socially undesirable behavior in the close-ended method, whereas the respondent must
verbalize the socially undesirable behavior in the open-ended methods.</p>
          <p>Hypothesis 4: Open-ended with probing will lead to more socially undesirable reports than
open-ended without probing. Probing may help an interviewer build rapport with a
respondent and uncover issues that respondents do not immediately discuss.</p>
        </sec>
      </sec>
      <sec id="sec-2-4">
        <title>Results</title>
        <sec id="sec-2-4-1">
          <title>Number of Endorsed Reasons</title>
          <p>In Table 3, we present the number of reasons endorsed by each experimental group,
separately for the three business practices. We report the percentage of respondents that
endorsed more than one reason, the percentage distribution of the number of reasons
endorsed, and the mean number of reasons. For “more than one reason” and “mean number
of reasons,” we use superscripts to highlight statistically significant differences (p &lt; .05) that
were obtained through post-hoct-tests.
3
90
7
0
0
5
0
95
5
&lt; 1
&lt; 1
6
3
90
3
3
0
9
0
91
3
3
3</p>
        </sec>
        <sec id="sec-2-4-2">
          <title>Close</title>
          <p>ended(n =
58)</p>
        </sec>
        <sec id="sec-2-4-3">
          <title>Open-ended</title>
          <p>with
probing(n =
67)</p>
          <p>Open-ended
without
probing(n =
62)
a Statistically significant difference from close-ended (p &lt; .05)b
Statistically significant difference from open-ended with probing (p &lt; .05)c
Statistically significant difference from open-ended without probing (p &lt;
.05)
In the reasons for not advertising panel, the results show that the close-ended design yielded
more reasons than both open-ended methods. In the close-ended group, 34% of
respondents provided more than one response, compared to 18% and 23% for the
open-ended groups with and without probing, respectively. The differences between the
close-ended group and both open-ended groups were statistically significant (p &lt; .05). The
full distribution shows that the close-ended group reported two reasons 25% of the time,
15.01.14 09:36
compared to 15% for the open-ended with probing and 21% for the open-ended without
probing group. The close-ended group also provided a higher mean number of reasons than
both open-ended groups (p &lt; .05). There was no statistically significant difference, however,
between the two open-ended groups in the number of endorsed reasons.</p>
          <p>We observed a similar pattern in the “switching supplier” panel. The close-ended group
reported more than one reason in 17% of cases, higher than the open-ended groups with
probing (7%) and without probing (6%), though these differences were only marginally
statistically significant (p &lt; .10). However, the close-ended group had a significantly higher (p
&lt; .05) mean number of reasons endorsed (1.3) compared to the open-ended with probing
group (1.0). The results in the “sharing storage” panel follow the same pattern. The
differences, however, are not statistically significant, likely due to the small sample sizes.
In sum, the close-ended method produced endorsements of more options compared to
open-ended methods, supporting Hypothesis 1. The results, however, do not provide support
for Hypothesis 2: probing had no effect on the number of reasons respondents endorse.</p>
        </sec>
        <sec id="sec-2-4-4">
          <title>Type of Responses Provided</title>
          <p>Next, we investigated how question design affected the number of socially undesirable
responses provided, separately by the three business practices.</p>
          <p>Reasons for Not Advertising
In Table 4, we show the reasons respondents provided for not advertising, separately by
experimental group. Several of these reasons are socially undesirable, such as the reason
that advertising might lead to an “increase government inspections or auditing.” This reason
is socially taboo because it indirectly refers to bribes: In developing countries such as
Ethiopia, advertising increases a businessʼ prominence, making it an easier target for
government officials to demand bribes through unnecessary inspections or audits.
Respondents may not endorse this reason because they prefer to avoid discussing about the
sensitive topic of bribes, and also to minimize being perceived as having paid bribes. Of
respondents in the close-ended group, 14% cited this reason, twice as high as the
open-ended with probing group (7%); this difference was statistically significant. Nine percent
of the open-ended without probing group mentioned this reason.</p>
          <p>Business is too new or
small
aStatistically significant difference from close-ended (p &lt; .05)bStatistically significant
difference from open-ended with probing (p &lt; .05)c Statistically significant difference
from open-ended without probing (p &lt; .05)
Table 4 also contains two other reasons that, while not socially undesirable, may be sensitive
to the method of questioning. These reasons include not advertising because it is too
complicated or because the respondent had never thought of advertising. Although these
reasons are not socially taboo, respondents may hesitate to report these reasons because
the reasons suggest that respondents have low levels of sophistication in running a
business. Never thinking of advertising was mentioned by 8% percent of respondents in the
close-ended group, more than the open-ended groups with probing (3%) and without probing
(5%). Similarly, 5% of the close-ended group said advertising was too complicated, higher
than both open-ended groups.</p>
          <p>These three results support Hypothesis 3, that close-ended questions will yield more socially
undesirable responses. However, there is no support for Hypothesis 4, that probing allows
interviewers to build a rapport with respondents and is more likely to encourage socially
undesirable reporting.</p>
          <p>Reasons for Not Switching Supplier
Table 5 shows the reasons respondents provided for not switching the business from whom
the respondent buys supplies or raw materials. The vast majority of respondents in all groups
reported not switching suppliers because they were satisfied with their current supplier.
There were no statistically significant differences in the reasons provided by the three
experimental groups. It is possible that the highly skewed distribution of these reasons may
account for the absence of an effect.</p>
          <p>Satisfied with current
supplier
Quality is too poor
Finding a new supplier
takes too long
In Table 6, we show the reasons that respondents provided for not sharing product storage
with another business. The sample sizes in this table are small because only respondents
who reported using storage (22% of the entire sample) were asked subsequent questions
about sharing storage.</p>
          <p>Not trusting other businesses is a socially taboo topic because community cohesion is valued
in Ethiopia and openly discussing distrust of others is discouraged. This reason was
endorsed by 30% of the close-ended group, significantly higher than the open-ended with
probing group (10%). Only 14% of the open-ended without probing group cited this reason.
This result supports Hypothesis 3 (close-ended responses will increase socially undesirable
reporting), but there is no support for Hypothesis 4 (that probing increases socially
undesirable reports). The experimental manipulation did not affect reports about “never
thought about it,” which contrasts with the results above for the reasons for not advertising.</p>
          <p>Donʼt need to share
Canʼt find other
businesses to share with
Cost savings are not
worth the effort
Laws prohibit sharing
a Statistically significant difference from close-ended (p &lt; .05)b Statistically significant
difference from open-ended with probing (p &lt; .05)c Statistically significant difference from
open-ended without probing (p &lt; .05)</p>
        </sec>
      </sec>
      <sec id="sec-2-5">
        <title>Discussion</title>
        <p>Our goal was to investigate best practices for asking respondents about reasons for their
behavior. Respondents endorsed more responses when asked close-ended (versus
open-ended) questions. This finding suggests that close-ended questions may spark greater
engagement with the subject matter because respondents are forced to consider each option
on its own, rather than reporting “top of mind” responses. Close-ended responses also
produced higher rates of socially undesirable responses, suggesting that close-ended
responses may help to elicit attitudes on sensitive topics. Providing socially undesirable
reasons through close-ended questions may reduce the stigma of the response. An
alternative hypothesis is that respondents had simply never thought of that reason before.
We leave it to future research to distinguish between these explanations.</p>
        <p>Second, probing did not affect the number of overall responses or the number of socially
undesirable responses provided. This lack of an effect is notable, particularly because the
professional interviewers had experience and training in probing. In fact, for two out of three
questions, probing leads to more “other” responses that could not be classified into existing
or new categories. It is possible that spending time on probing may not be an efficient use of
interviewersʼ efforts, particularly because interviewer probing may also introduce additional
variance into estimates. However, additional studies based on larger sample sizes are
needed to replicate this null finding, particularly for more difficult questions where probing
might be more effective. Future research could also investigate what types of probes are
most productive at eliciting sensitive data from respondents.</p>
        <p>In sum, our results provide tentative support for the idea that close-ended questions without
probing are the preferred method of asking respondents to provide reasons for their
behavior, at least for this population and topic. We are limited, however, in that we do not
have a gold standard that could specify which of the three designs produces the most valid
data. Future research should investigate the validity of different methods, particularly the
assumption that the additional responses provided by the close-ended questions are
meaningful. Researchers should also consider the possibility is that there is no method that
provides “true” reports, but simply that the three methods collect different types of data. For
example, open-ended questions may produce reasons that are immediately accessible in
respondentʼs minds, whereas close-ended questions can obtain reactions to issues
respondents rarely consider in their day-to-day lives. Cognitive interviewing could be useful
15.01.14 09:36
in understanding how respondents approach the response task of questions that ask about
reasons for behavior.</p>
        <p>Our results demonstrate the feasibility of asking respondents questions about reasons for
their behavior, but also raise questions about the validity of these data. Rates of item
non-response were less than 1% for these items, suggesting that individuals are willing to
provide reasons for their behavior. However, our results cannot show whether the reasons
provided are accurate. Respondents may intentionally misreport reasons for their behavior or
otherwise rationalize their behavior. Citing reasons for behavior may be cognitively
burdensome, particularly when respondents do not often consciously think about why they do
(or do not) engage in behaviors. For example, asking about topics a respondent has not
considered before may encourage the respondent to create an answer on the spot, leading
to inconsistent responses across items designed to measure similar concepts (Wilson 2013).
Alternatively, interviewers may make errors when classifying responses. Because of these
limitations, we view respondent self-reports about why they engage in behaviors as one of
several possible research methods (in addition to qualitative research and experiments) that
policymakers could use when designing policies or programs to modify behavior.
We encourage future research on the best practices and validity of asking respondents
questions about reasons for their behavior, particularly in larger samples in different
populations and substantive content areas. Research could fruitfully investigate various
designs for designing these questions, such as using showcards for close-ended lists or
coding verbatim open-ended responses. Another promising avenue for future research is to
study how to ask questions about why individuals do engage in behaviors rather than why
they do not. We believe that future research about if and when it is appropriate to ask these
questions will ultimately benefit policymakers who rely on social scientists to explain why
individuals engage in particular behaviors.</p>
        <p>Appendix_Question Wording
[1] An alternative method is collecting verbatim responses to open-ended questions and then
coding the verbatim responses post-hoc, ideally with multiple coders. Although this method
may reduce coding errors, it is also time and labor intensive. The survey we analyze in this
paper was designed to be a rapid and low-cost survey for use in settings such as Ethiopia,
so it used immediate classification methods by interviewers.
15.01.14 09:36</p>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref id="ref1">
        <mixed-citation>
          1.
          <article-title>Centers for Disease Control and Prevention (</article-title>
          <year>2012</year>
          ).
          <source>National Health Interview Survey 2012 Questionnaires, English. Retrieved electronically on 20 December</source>
          ,
          <year>2013</year>
          from http://www.cdc.gov/nchs/nhis/quest_data_related_
          <year>1997</year>
          <article-title>_forward</article-title>
          .htm.
        </mixed-citation>
      </ref>
      <ref id="ref2">
        <mixed-citation>
          2.
          <string-name>
            <surname>Converse</surname>
            ,
            <given-names>J. M.</given-names>
          </string-name>
          ,
          <string-name>
            <given-names>and H.</given-names>
            <surname>Schuman</surname>
          </string-name>
          .
          <year>1974</year>
          . Conversations at Random:
          <article-title>Survey Research as Interviewers See It</article-title>
          . Hoboken, NJ: John Wiley &amp; Sons.
        </mixed-citation>
      </ref>
      <ref id="ref3">
        <mixed-citation>
          3.
          <string-name>
            <surname>Currivan</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <given-names>A.</given-names>
            <surname>Nyman</surname>
          </string-name>
          ,
          <string-name>
            <given-names>C. F.</given-names>
            <surname>Turner</surname>
          </string-name>
          , and
          <string-name>
            <given-names>L.</given-names>
            <surname>Biener</surname>
          </string-name>
          .
          <year>2004</year>
          .
          <article-title>Does Telephone Audio Computer-Assisted Self-Interviewing Improve the Accuracy of Prevalence Estimates of Youth Smoking? Evidence from the UMass Tobacco Study</article-title>
          .
          <source>Public Opinion Quarterly</source>
          <volume>68</volume>
          :
          <fpage>542</fpage>
          -
          <lpage>564</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref4">
        <mixed-citation>
          4.
          <string-name>
            <surname>Fowler</surname>
            ,
            <given-names>F. J.</given-names>
          </string-name>
          , and
          <string-name>
            <given-names>T.</given-names>
            <surname>Mangione</surname>
          </string-name>
          .
          <year>1990</year>
          .
          <article-title>Standardized Survey Interviewing: Minimizing Interviewer-Related Error</article-title>
          . Newbury Park: Sage.
        </mixed-citation>
      </ref>
      <ref id="ref5">
        <mixed-citation>
          5.
          <string-name>
            <surname>Fowler</surname>
            ,
            <given-names>F. J.</given-names>
          </string-name>
          <year>1995</year>
          .
          <article-title>Improving Survey Questions: Design and Evaluation</article-title>
          . Thousand Oaks, CA: Sage Publications.
        </mixed-citation>
      </ref>
      <ref id="ref6">
        <mixed-citation>
          6.
          <string-name>
            <surname>Geer</surname>
            ,
            <given-names>J. G.</given-names>
          </string-name>
          <year>1988</year>
          .
          <string-name>
            <given-names>What</given-names>
            <surname>Do</surname>
          </string-name>
          Open-Ended
          <source>Questions Measure? Public Opinion Quarterly</source>
          <volume>52</volume>
          :
          <fpage>365</fpage>
          -
          <lpage>367</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref7">
        <mixed-citation>
          7.
          <string-name>
            <surname>Geer</surname>
            ,
            <given-names>J. G.</given-names>
          </string-name>
          <year>1991</year>
          . Do Open-Ended
          <source>Questions Measure “Salient” Issues? Public Opinion Quarterly</source>
          <volume>55</volume>
          :
          <fpage>360</fpage>
          -
          <lpage>370</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref8">
        <mixed-citation>
          8.
          <string-name>
            <surname>Groves</surname>
            ,
            <given-names>R.M.</given-names>
          </string-name>
          ,
          <string-name>
            <given-names>F.J.</given-names>
            <surname>Fowler</surname>
          </string-name>
          ,
          <string-name>
            <given-names>M. P.</given-names>
            <surname>Couper</surname>
          </string-name>
          ,
          <string-name>
            <given-names>J. M.</given-names>
            <surname>Lepkowski</surname>
          </string-name>
          , E. Singer, and
          <string-name>
            <given-names>R.</given-names>
            <surname>Tourangeau</surname>
          </string-name>
          .
          <year>2009</year>
          .
          <string-name>
            <given-names>Survey</given-names>
            <surname>Methodology</surname>
          </string-name>
          . Hoboken, NJ: John Wiley &amp; Sons.
        </mixed-citation>
      </ref>
      <ref id="ref9">
        <mixed-citation>
          9.
          <string-name>
            <surname>Heckathorn</surname>
            ,
            <given-names>D. D.</given-names>
          </string-name>
          <year>1997</year>
          .
          <article-title>Respondent-Driven Sampling: A New Approach to the Study of Hidden Populations</article-title>
          .
          <source>Social Problems</source>
          ,
          <volume>44</volume>
          :
          <fpage>174</fpage>
          -
          <lpage>199</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref10">
        <mixed-citation>
          10.
          <string-name>
            <surname>Pasek</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          , and
          <string-name>
            <given-names>J. A.</given-names>
            <surname>Krosnick</surname>
          </string-name>
          .
          <year>2010</year>
          .
          <article-title>Optimizing Survey Questionnaire Design in Political Science: Insights From Psychology</article-title>
          . In Oxford Handbook of American Elections and Political Behavior, ed.
          <source>J. Leighley</source>
          ,
          <volume>27</volume>
          -
          <fpage>50</fpage>
          . Oxford: Oxford University Press.
        </mixed-citation>
      </ref>
      <ref id="ref11">
        <mixed-citation>
          11.
          <string-name>
            <surname>Schober</surname>
            ,
            <given-names>M. F.</given-names>
          </string-name>
          <year>1998</year>
          .
          <article-title>Making Sense of Questions: An Interactional Approach</article-title>
          . In Cognition and Survey Research, eds. M. G. Sirken,
          <string-name>
            <given-names>D. J.</given-names>
            <surname>Herrmann</surname>
          </string-name>
          ,
          <string-name>
            <given-names>S.</given-names>
            <surname>Schechter</surname>
          </string-name>
          ,
          <string-name>
            <given-names>N.</given-names>
            <surname>Schwarz</surname>
          </string-name>
          ,
          <string-name>
            <given-names>J. M.</given-names>
            <surname>Tanur</surname>
          </string-name>
          , and
          <string-name>
            <given-names>R.</given-names>
            <surname>Tourangeau</surname>
          </string-name>
          . New York: Wiley.
        </mixed-citation>
      </ref>
      <ref id="ref12">
        <mixed-citation>
          12.
          <string-name>
            <surname>Schober</surname>
            ,
            <given-names>M. F.</given-names>
          </string-name>
          , and
          <string-name>
            <given-names>F. G.</given-names>
            <surname>Conrad</surname>
          </string-name>
          .
          <year>1997</year>
          .
          <source>Does Conversational Interviewing Reduce Survey Measurement Error? Public Opinion Quarterly</source>
          <volume>61</volume>
          :
          <fpage>576</fpage>
          -
          <lpage>602</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref13">
        <mixed-citation>
          13.
          <string-name>
            <surname>Schober</surname>
            ,
            <given-names>M. F.</given-names>
          </string-name>
          , and
          <string-name>
            <given-names>F. G.</given-names>
            <surname>Conrad</surname>
          </string-name>
          .
          <year>2002</year>
          .
          <article-title>A Collaborative View of Standardized Survey Interviews. In Standardization and Tacit Knowledge: Interaction and Practice in the Survey Interview</article-title>
          , eds. D.
          <string-name>
            <surname>Maynard</surname>
            ,
            <given-names>H.</given-names>
          </string-name>
          <string-name>
            <surname>Houtkoop-Steenstra</surname>
            ,
            <given-names>N. C.</given-names>
          </string-name>
          <string-name>
            <surname>Schaeffer</surname>
            , and
            <given-names>J. van der</given-names>
          </string-name>
          <string-name>
            <surname>Zouwen</surname>
          </string-name>
          . New York: Wiley.
        </mixed-citation>
      </ref>
      <ref id="ref14">
        <mixed-citation>
          14.
          <string-name>
            <surname>Schaeffer</surname>
            ,
            <given-names>N. C.</given-names>
          </string-name>
          , and
          <string-name>
            <given-names>D. W.</given-names>
            <surname>Maynard</surname>
          </string-name>
          .
          <year>2008</year>
          .
          <article-title>The Contemporary Standardized Survey Interview for Social Research</article-title>
          . In Envisioning the Survey Interview of the Future, ed. F. G. Conrad and
          <string-name>
            <given-names>M.</given-names>
            <surname>Schober</surname>
          </string-name>
          . New York: Wiley.
        </mixed-citation>
      </ref>
      <ref id="ref15">
        <mixed-citation>
          15.
          <string-name>
            <surname>Smyth</surname>
          </string-name>
          ,
          <string-name>
            <surname>J.D.</surname>
            ,
            <given-names>D.A.</given-names>
          </string-name>
          <string-name>
            <surname>Dillman</surname>
            ,
            <given-names>L. M.</given-names>
          </string-name>
          <string-name>
            <surname>Christian</surname>
            , and
            <given-names>M.J.</given-names>
          </string-name>
          <string-name>
            <surname>Stern</surname>
          </string-name>
          .
          <year>2006</year>
          .
          <article-title>Comparing Check-all and Forced-Choice Question Formats in Web Surveys</article-title>
          .
          <source>Public Opinion Quarterly</source>
          ,
          <volume>70</volume>
          :
          <fpage>66</fpage>
          -
          <lpage>77</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref16">
        <mixed-citation>
          16.
          <string-name>
            <surname>Tourangeau</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          and
          <string-name>
            <surname>Smith</surname>
            ,
            <given-names>T. W.</given-names>
          </string-name>
          <year>1996</year>
          .
          <article-title>Asking Sensitive Questions: The Impact of Data Collection Mode, Question Format, and Question Context</article-title>
          .
          <source>Public Opinion Quarterly</source>
          ,
          <volume>60</volume>
          :
          <fpage>275</fpage>
          -
          <lpage>305</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref17">
        <mixed-citation>
          17.
          <string-name>
            <surname>United States Census Bureau</surname>
          </string-name>
          (
          <year>2010</year>
          ).
          <article-title>Current Population Survey Voting and Registration Supplement Questionnaire</article-title>
          , November,
          <year>2010</year>
          .
          <source>Retrieved electronically on 20 December</source>
          ,
          <year>2013</year>
          from http://www.census.gov/cps/methodology/techdocs.html.
        </mixed-citation>
      </ref>
      <ref id="ref18">
        <mixed-citation>
          18. Wilson,
          <string-name>
            <surname>S.</surname>
          </string-name>
          <year>2010</year>
          .
          <article-title>Cognitive Interview Evaluation of the 2010 National Health Interview Survey Supplement on Cancer Screenings &amp; Survivorship: Results of interviews conducted October -</article-title>
          December,
          <year>2008</year>
          . National Center for Health Statistics. Hyattsville, MD.
        </mixed-citation>
      </ref>
      <ref id="ref19">
        <mixed-citation>
          19.
          <string-name>
            <surname>Willson</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          <year>2013</year>
          .
          <article-title>Cognitive Interview Evaluation of the Federal Statistical System Trust Monitoring Survey, Round 1: Results of interviews conducted in October, 2011</article-title>
          . National Center for Health Statistics. Hyattsville, MD.
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>